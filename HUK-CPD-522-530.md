## HUK CPD upgrade 5.2.2 to 5.3.0

## Author: Alex Kuan (alex.kuan@ibm.com)

From:

```
CPD: 5.2.2
OCP: 4.18
Storage: Spectrum Scale 2.10.1
Internet: proxy
Private container registry: yes
Components: ibm-cert-manager,ibm-licensing,cpfs,cpd_platform,datastage_ent_plus,ws_pipelines,wkc,mantaflow
```

To:

```
CPD: 5.3.0
OCP: 4.18
Storage: Spectrum Scale 2.10.1
Internet: proxy
Private container registry: yes
Components: ibm-cert-manager,ibm-licensing,cpfs,cpd_platform,datastage_ent_plus,ws_pipelines,wkc,mantaflow
```

Upgrade flow and steps

```
1. CPD 5.2.2 pre-check
2. Update cpd-cli and environment variables script for 5.3.0
3. Backup CPD 5.2.2 CRs, cp4d and cp4d-operators namespaces
4. Revert any patches on the current installation
5. Upgrade shared cluster components (ibm-cert-manager,ibm-licensing)
6. Prepare to upgrade an instance of IBM Software Hub
7. Upgrade an instance of IBM Software Hub
8. Upgrade CPD services (datastage_ent_plus,ws_pipelines,wkc,mantaflow)
9. Upgrade the cpdbr service
10. Upgrade privileged monitors
11. Upgrade the configuration admission controller webhook
12. Validate CPD upgrade (customer acceptance test)
```


## 1. CPD 5.3.0 pre-check

Use a client workstation with internet (bastion or infra node) to download OCP and CPD images, and confirm the OS level, ensuring the OS is RHEL 8/9

```
cat /etc/redhat-release
```

Test internet connection, and make sure the output from the target URL and it can be connected successfully:

```
curl -v https://github.com/IBM
```

Prepare customer's IBM entitlement key

Log in to <https://myibm.ibm.com/products-services/containerlibrary> with the IBMid and password that are associated with the entitled software.

On the Get entitlement key tab, select Copy key to copy the entitlement key to the clipboard.

Save the API key in a text file.

Make sure free disk space more than 500 GB (to download images and pack the images into a tar ball)

```
df -lh
```

Collect OCP and CPD cluster information

Log into OCP cluster from bastion node

```
oc login $(oc whoami --show-server) -u kubeadmin -p <kubeadmin-password>
```

Review OCP version

```
oc get clusterversion
```

Review storage classes

```
oc get sc
```

Review OCP cluster status

Make sure all nodes are in ready status

```
oc get nodes
```

Make sure all mc are in correct status, UPDATED all True, UPDATING all False, DEGRADED all False

```
oc get mcp
```

Make sure all co are in correct status, AVAILABLE all True, PROGRESSING all False, DEGRADED all False

```
oc get co
```

Make sure there are no unhealthy pods, if there are, please open an IBM support case to fix them

```
oc get po -A -owide | egrep -v '([0-9])/\1' | egrep -v 'Completed' 
```

Get CPD installed projects

```
oc get pod -A | grep zen
```

Get CPD version and installed components

```
cpd-cli manage login-to-ocp \
--username=${OCP_USERNAME} \
--password=${OCP_PASSWORD} \
--server=${OCP_URL}
```

```
cpd-cli manage get-cr-status --cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS}
```

or

```
cpd-cli manage list-deployed-components --cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS}
```

Check the scheduling service, if it is installed but not in ibm-common-services project, uninstall it

```
oc get scheduling -A
```

Check install plan is automatic

```
oc get ip -A
```

Check the spec of each CPD custom resource, remove any patches before upgrading

```
oc project ${PROJECT_CPD_INST_OPERANDS}
```

```
for i in $(oc api-resources | grep cpd.ibm.com | awk '{print $1}'); do echo "************* $i *************"; for x in $(oc get $i --no-headers | awk '{print $1}'); do echo "--------- $x ------------"; oc get $i $x -o jsonpath={.spec} | jq; done ; done
```

Probe IBM registry (if required)

```
podman login cp.icr.io -u cp -p ${IBM_ENTITLEMENT_KEY}
```

If the IBM Certificate manager (ibm-cert-manager) is installed on your cluster, use the following steps to migrate your certificates from the IBM Certificate manager to the Red Hat OpenShift certificate manager (cert-manager Operator)

```
https://www.ibm.com/docs/en/software-hub/5.3.x?topic=upgrading-migrating-red-hat-openshift-certificate-manager
```


## 2. Update cpd-cli, Helm CLI and environment variables script for 5.3.0

Download and unpack the latest cpd-cli release

```
wget https://github.com/IBM/cpd-cli/releases/download/v14.3.0_refresh_2/cpd-cli-linux-EE-14.3.0.tgz && gzip -d cpd-cli-linux-EE-14.3.0.tgz && tar -xvf cpd-cli-linux-EE-14.3.0.tar && rm -rf cpd-cli-linux-EE-14.3.0.tar
```

Add the cpd-cli to your PATH variable, for example

```
export PATH=/root/cpd-cli-linux-EE-14.3.0-2819:$PATH
```

Update your environment variables script

```
vi cpd_vars.sh
```

Update the Version field and save your changes

```
VERSION=5.3.0
```

Source your environment variables

```
source cpd_vars.sh
```

Launch olm-utils-play-v4 container

```
cpd-cli manage restart-container
```

Download and install the latest release of Helm CLI

```
https://github.com/helm/helm/releases
```

Or run a wget command such as this one

```
wget https://get.helm.sh/helm-v4.0.4-linux-amd64.tar.gz && gzip -d helm-v4.0.4-linux-amd64.tar.gz  && tar -xvf helm-v4.0.4-linux-amd64.tar && rm -rf helm-v4.0.4-linux-amd64.tar
```

Replace old Helm CLI files in /usr/local/bin/helm on thet bastion node

```
mv linux-amd64/helm /usr/local/bin/helm
chmod +x /usr/local/bin/helm
```

Check the Helm CLI version

```
helm version
```

Output should look like this

```
version.BuildInfo{Version:"v4.0.4", GitCommit:"8650e1dad9e6ae38b41f60b712af9218a0d8cc11", GitTreeState:"clean", GoVersion:"go1.25.5", KubeClientVersion:"v1.34"}
```

If the scheduling service is installed on the cluster, you must update the cluster-scoped resources and create an image pull secret in the project where you plan to install the scheduling service

```
https://www.ibm.com/docs/en/software-hub/5.3.x?topic=pyc-updating-cluster-scoped-resources-shared-cluster-components
```

```
https://www.ibm.com/docs/en/software-hub/5.3.x?topic=pyc-creating-image-pull-secrets-shared-cluster-components-1
```


## 3. Backup CPD 5.2.2 CRs, cp4d and cp4d-operators namespaces

Create a new directory and store the output of the following commands in that directory

```
mkdir cpdbackup && cd cpdbackup && oc project ${PROJECT_CPD_INST_OPERANDS}
for i in $(oc api-resources | grep cpd.ibm.com | awk '{print $1}'); do echo "************* $i *************"; for x in $(oc get $i --no-headers | awk '{print $1}'); do echo "--------- $x ------------"; oc get $i $x -oyaml > bak-$x.yaml; done ; done
```

**Note: The following 'oc adm' commands can be time-consuming and should be collected during the pre-upgrade Health Check activity**

Backup the current state of operands in your backup folder of choice:

```
mkdir operandsbackup && cd operandsbackup && oc adm inspect -n ${PROJECT_CPD_INST_OPERANDS} --dest-dir=source-${PROJECT_CPD_INST_OPERANDS} $(oc api-resources --namespaced=true --verbs=get,list --no-headers -o name | tr '\n' ',' | sed 's/,$//')
```

Backup the current state of operators in your backup folder of choice:

```
mkdir operatorsbackup && cd operatorsbackup && oc adm inspect -n ${PROJECT_CPD_INST_OPERATORS} --dest-dir=source-${PROJECT_CPD_INST_OPERATORS} $(oc api-resources --namespaced=true --verbs=get,list --no-headers -o name | tr '\n' ',' | sed 's/,$//')
```

## 4. Revert Patches And Hot Fixes (skip for now...)


## 5. Upgrade shared cluster components (ibm-licensing, ibm-scheduling) (est. 3 minutes)

Determine which project the License Service is in

```
oc get deployment -A | grep ibm-licensing-operator
```

Upgrade the License Service

The License Service is in the ${PROJECT_LICENSE_SERVICE} project

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster

```
${CPDM_OC_LOGIN}
```

Upgrade shared cluster components

```
cpd-cli manage apply-cluster-components \
--release=${VERSION} \
--license_acceptance=true \
--licensing_ns=${PROJECT_LICENSE_SERVICE} \
--case_download=false
```

Wait for the cpd-cli to return the following message before proceeding to the next step:

[SUCCESS] ... The apply-cluster-components command ran successfully

Confirm that the License Service pods are Running or Completed

```
oc get pods --namespace=${PROJECT_LICENSE_SERVICE}
```


## 6. Prepare to upgrade an instance of IBM Software Hub (est. 3 minutes)

### [Check the health of your cluster before proceeding with the upgrade](https://www.ibm.com/docs/en/software-hub/5.3.x?topic=puish-checking-health-your-cluster-1):

```
cpd-cli health operators \
--control_plane_ns=${PROJECT_CPD_INST_OPERANDS} \
--operator_ns=${PROJECT_CPD_INST_OPERATORS}
```

Results should read "SUCCESS..."

```
cpd-cli health operands \
--control_plane_ns=${PROJECT_CPD_INST_OPERANDS} \
--include_ns=${PROJECT_CPD_INST_OPERATORS}
```

Results should read "SUCCESS..."

```
cpd-cli health cluster
```

Results should read "SUCCESS..."

```
cpd-cli health nodes
```

Results should read "SUCCESS..."

### [Update the cluster-scoped resources for the platform and services](https://www.ibm.com/docs/en/software-hub/5.3.x?topic=puish-updating-cluster-scoped-resources-instance):

Generate the cluster_scoped_resources.yaml file. The command requires the CASE packages to be on the workstation (est. 3-4 minutes)

```
cpd-cli manage case-download \
--components=${COMPONENTS} \
--release=${VERSION} \
--operator_ns=${PROJECT_CPD_INST_OPERATORS} \
--cluster_resources=true
```

Change to the work directory

```
cd cpd-cli-workspace/olm-utils-workspace/work
```

Log in to Red Hat® OpenShift® Container Platform as a cluster administrator

```
${OC_LOGIN}
```

Apply the cluster-scoped resources for the from the cluster_scoped_resources.yaml file:

```
oc apply -f cluster_scoped_resources.yaml \
--server-side \
--force-conflicts
```

Optional: If you want a record of the resources that you generated, rename the cluster_scoped_resources.yaml

```
mv cluster_scoped_resources.yaml ${VERSION}-${PROJECT_CPD_INST_OPERATORS}-cluster_scoped_resources.yaml
```

### [Confirm whether the NamespaceScope operator is using the minimum RBAC, run the following command](https://www.ibm.com/docs/en/software-hub/5.3.x?topic=puish-reauthorizing-namespacescope-operator-minimum-rbac) (est. 1-2 minutes):

To confirm whether the NamespaceScope operator is using the minimum RBAC, run the following command:

```
oc get role nss-managed-role-from-${PROJECT_CPD_INST_OPERATORS} \
-n ${PROJECT_CPD_INST_OPERATORS} \
-o json | jq 'any(.rules[].apiGroups[]; . == "*")'
```

If the command returns TRUE, the operator does not need to be reauthorized, move on to the next step

If the command returns FALSE, the operator is using the minimum RBAC and must be reauthorized, follow the procedure here

```
https://www.ibm.com/docs/en/software-hub/5.3.x?topic=puish-reauthorizing-namespacescope-operator-minimum-rbac
```

### [Confirm if you need to reauthorize an instance administrator with the minimum RBAC to upgrade components](https://www.ibm.com/docs/en/software-hub/5.3.x?topic=puish-reauthorizing-user-minimum-rbac) (est. 1-2 minutes):

### [Apply entitlements](https://www.ibm.com/docs/en/software-hub/5.2.x?topic=puish-applying-your-entitlements-3) (est. 1-2 minutes):

Run the apply-entitlement command for each solution that is installed or that you plan to install in this instance

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:

```
${CPDM_OC_LOGIN}
```

Apply entitlements for CP4D (includes WKC and Mantaflow):

NON-PR and WA
```
cpd-cli manage apply-entitlement \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--entitlement=cpd-enterprise \
--production=false
```

PR
```
cpd-cli manage apply-entitlement \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--entitlement=cpd-enterprise
```

Apply entitlements for DataStage:

```
export LICENSE_NAME=datastage-plus
```

NON-PR and WA
```
cpd-cli manage apply-entitlement \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--entitlement=${LICENSE_NAME} \
--production=false
```

PR
```
cpd-cli manage apply-entitlement \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--entitlement=${LICENSE_NAME}
```


## 7. Upgrade an instance of IBM Software Hub

### Create image pull secrets for an instance of IBM Software Hub:

You must create an image pull secret in the projects that are associated with an instance of IBM Software Hub

The image pull secrets ensure that any components that use Helm have access to the credentials for pulling images

Log in to Red Hat® OpenShift® Container Platform as a user with sufficient permissions to complete the task.

```
${OC_LOGIN}
```

Set your $IMAGE_PULL_SECRET and $IMAGE_PULL_CREDENTIALS environment variables
```
export IMAGE_PULL_SECRET=<any name should work here> # i.e. ibm-entitled-regcred
export IMAGE_PULL_CREDENTIALS=$(echo -n "cp:$IBM_ENTITLEMENT_KEY" | base64 -w 0)
```

Create a file named dockerconfig.json based on where your cluster pulls images from

For private container registry
```
cat <<EOF > dockerconfig.json 
{
  "auths": {
    "PRIVATE_REGISTRY_LOCATION": {
      "auth": "${IMAGE_PULL_CREDENTIALS}"
    }
  }
}
EOF
```

Create the image pull secret in the operators project for the instance:
```
oc create secret docker-registry ${IMAGE_PULL_SECRET} \
--from-file ".dockerconfigjson=dockerconfig.json" \
--namespace=${PROJECT_CPD_INST_OPERATORS}
```

Create the image pull secret in the operands project for the instance:
```
oc create secret docker-registry ${IMAGE_PULL_SECRET} \
--from-file ".dockerconfigjson=dockerconfig.json" \
--namespace=${PROJECT_CPD_INST_OPERANDS}
```

If your instance includes tethered projects, create the image pull secret in each project that you plan to tether to the operands project:
```
oc create secret docker-registry ${IMAGE_PULL_SECRET} \
--from-file ".dockerconfigjson=dockerconfig.json" \
--namespace=${PROJECT_CPD_INSTANCE_TETHERED}
```

Repeat this step for each tethered project that is associated with this instance of IBM Software Hub

Now that you've created the image pull secrets for this instance of IBM Software Hub, you're ready to upgrade IBM Software Hub

### [Upgrade the required operators and custom resources for the instance](https://www.ibm.com/docs/en/software-hub/5.3.x?topic=uish-upgrading-software-hub):

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:

```
${CPDM_OC_LOGIN}
```

Review the license terms for the software that you plan to install:

```
cpd-cli manage get-license \
--release=${VERSION} \
--license-type=EE
```

Upgrade the required operators and custom resources for the instance (est. 36 minutes):

```
cpd-cli manage install-components \
--license_acceptance=true \
--components=cpd_platform \
--release=${VERSION} \
--operator_ns=${PROJECT_CPD_INST_OPERATORS} \
--instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--image_pull_prefix=${IMAGE_PULL_PREFIX} \
--image_pull_secret=${IMAGE_PULL_SECRET} \
--run_storage_tests=false \
--upgrade=true \
--case_download=false
```

Monitor the upgrade progress of the custom resources with the following commands:

```
oc get ZenService lite-cr -n cp4d -o yaml
```

```
oc get Ibmcpd ibmcpd-cr -n cp4d -o yaml
```

Wait for the cpd-cli to return the following message before proceeding to the next step:

[SUCCESS]... The install-components command ran successfully


## 8. Upgrade CPD services (datastage_ent_plus,ws_pipelines,wkc,mantaflow)

### Upgrade CCS custom resource (est. 30 minutes):
```
cpd-cli manage install-components \
--license_acceptance=true \
--components=ccs \
--release=${VERSION} \
--operator_ns=${PROJECT_CPD_INST_OPERATORS} \
--instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--image_pull_prefix=${IMAGE_PULL_PREFIX} \
--image_pull_secret=${IMAGE_PULL_SECRET} \
--upgrade=true \
--case_download=false
```

Monitor ccs pods for any errors
```
oc get pods -n cpd-instance | grep -E 'ccs|runtime'
```

Monitor ccs-cr yaml for progress
```
oc get ccs ccs-cr -o yaml
```

```
oc get ccs ccs-cr -o yaml | grep progress
```

**Potential ImagePullBackOff Issue Encountered during CCS upgrade**

You might encounter an ImagePullBackOff problem with the following CCS pods
```
runtime-assemblies-operator-7b6f77f7b9-8qrk4                  0/1     ImagePullBackOff             0             34m
runtime-manager-api-64bccc9497-nwhgt                          0/1     ImagePullBackOff             0             33m
```

Describe the pod and observe the following error messages
```
Failed to pull image "cp.icr.io/cp/cpd/runtime-manager-api@sha256:e6f927bac8b8559f613a777a1050954794ecb376ff3b253a216868d3234320f4": initializing source docker://cp.icr.io/cp/cpd/runtime-manager-api@sha256:e6f927bac8b8559f613a777a1050954794ecb376ff3b253a216868d3234320f4: unable to retrieve auth token: invalid username/password: unauthorized: Authorization required.
```

There is a Service Account in use, attempting to pull the images from cp.icr.io
```
Service Account: runtime-manager-api

cp.icr.io/cp/cpd/...
```

Check whether the Service Account has the pull secret
```
oc get sa runtime-manager-api -n cpd-instance -o yaml
```

Attach the entitlement secret to the Service Account
```
oc patch sa runtime-manager-api \
  -n cpd-instance \
  -p '{"imagePullSecrets":[{"name":"ibm-entitled-regcred"}]}'
```

Do the same for the other Service Account
```
oc describe pod runtime-assemblies-operator-7b6f77f7b9-8qrk4 -n cpd-instance | grep "Service Account"
```

Check the Service Account
```
oc get sa runtime-assemblies-operator -n cpd-instance -o yaml
```

Patch it to include the entitlement secret
```
oc patch serviceaccount runtime-assemblies-operator -n cpd-instance \
  -p '{"imagePullSecrets": [{"name": "runtime-assemblies-operator-dockercfg-n7f9x"},{"name": "ibm-entitled-regcred"}]}'
```

Delete the stuck pods and monitor for them to spin up again
```
# Delete the runtime-manager-api pod
oc delete pod runtime-manager-api-64bccc9497-nwhgt -n cpd-instance

# Delete the runtime-assemblies-operator pod
oc delete pod runtime-assemblies-operator-dockercfg-n7f9x -n cpd-instance
```

Monitor the runtime pods again, they should be up and running
```
oc get po | grep runtime-
```

```
runtime-assemblies-operator-7b6f77f7b9-x5gj8                  1/1     Running
runtime-manager-api-64bccc9497-s9ctp                          1/1     Running 
```

After you've confirmed CCS is fully upgraded, you can proceed with the upgrade of WKC


### Upgrade WKC custom resource (est. 40 minutes not including patching):

When you upgrade IBM Knowledge Catalog, the options that you specified when you installed IBM Knowledge Catalog are used

Ensure you have specified the correct installation options in the wkc-install-options.yml in the cpd-cli work directory (For example: /data/cpdcli/cpd-cli-workspace/olm-utils-workspace/work)

Change directories to your cpd-cli-workspace/olm-utils-workspace/work directory
```
cd cpd-cli-workspace/olm-utils-workspace/work/
```

Create the wkc-install-options.yml for IBM Knowledge Catalog - The following values were extracted from the latest revision of HUK internal runbook:
```
################################################################################
# IBM Knowledge Catalog parameters
################################################################################
custom_spec:
  wkc:
    enableDataQuality: True
    enableSemanticAutomation: False
    enableKnowledgeGraph: True
    useFDB: True
```

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster
```
${CPDM_OC_LOGIN}
```

Set the IKC_TYPE environment variable
```
export IKC_TYPE=wkc
```

Update the custom resource for IBM Knowledge Catalog
```
cpd-cli manage install-components \
--license_acceptance=true \
--components=${IKC_TYPE} \
--release=${VERSION} \
--operator_ns=${PROJECT_CPD_INST_OPERATORS} \
--instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--image_pull_prefix=${IMAGE_PULL_PREFIX} \
--image_pull_secret=${IMAGE_PULL_SECRET} \
--param-file=/tmp/work/wkc-install-options.yml \
--upgrade=true \
--case_download=false
```

Upgrading wkc also upgrades several dependencies in the following order
```
The install order for Level 0 is: ['platform-config']
The install order for Level 1 is: ['opencontent_opensearch']
The install order for Level 2 is: ['ccs'] (should already be installed from the previous step)
The install order for Level 3 is: ['datarefinery', 'opencontent_fdb', 'datastage_ent', 'analyticsengine', 'ibm_neo4j']
The install order for Level 4 is: ['wkc']
```

IBM Knowledge Catalog is upgraded when the apply-cr command returns:

[SUCCESS]... The apply-cr command ran successfully

If you want to confirm that the custom resource status is Completed, you can run the cpd-cli manage get-cr-status command:

```
cpd-cli manage get-cr-status \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--components=${IKC_TYPE} \
--case_download=false
```

Monitor for relevant pods, look for any pods in error statuses
```
oc get pods -n ${PROJECT_CPD_INST_OPERANDS} | grep -E "wkc-|wdp-|catalog-|governance-|dc-main|iis-|finley-|dq-"
```

```
oc get po -n cpd-instance | grep -E 'wkc|wdp' | grep -E 'Error|Crash|BackOff'
```

Monitor the status of the WKC custom resource for 'Completed':
```
oc get WKC wkc-cr -o yaml
```


### Upgrade MANTA Automated Data Lineage custom resource (est. 6-7 minutes):

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:
```
${CPDM_OC_LOGIN}
```

Update the custom resource for MANTA Automated Data Lineage:
```
cpd-cli manage install-components \
--license_acceptance=true \
--components=mantaflow \
--release=${VERSION} \
--operator_ns=${PROJECT_CPD_INST_OPERATORS} \
--instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--image_pull_prefix=${IMAGE_PULL_PREFIX} \
--image_pull_secret=${IMAGE_PULL_SECRET} \
--upgrade=true \
--case_download=false
```

Monitor relevant mantaflow pods
```
oc get po | grep manta
```

Monitor the status of the MANTA custom resource:

```
oc get MantaFlow mantaflow-wkc -o yaml
```

MANTA Automated Data Lineage is upgraded when the apply-cr command returns:

[SUCCESS]... The apply-cr command ran successfully

If you want to confirm that the custom resource status is Completed, you can run the cpd-cli manage get-cr-status command:

```
cpd-cli manage get-cr-status \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--components=mantaflow \
--case_download=false
```

Mantaflow is upgraded when the install-components command returns:

[SUCCESS]... The install-components command ran successfully


### Upgrade IBM Orchestration Pipelines custom resource (est. 12 minutes not including patching):

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:

```
${CPDM_OC_LOGIN}
```

Update the custom resource for IBM Orchestration Pipelines:

```
cpd-cli manage install-components \
--license_acceptance=true \
--components=ws_pipelines \
--release=${VERSION} \
--operator_ns=${PROJECT_CPD_INST_OPERATORS} \
--instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--image_pull_prefix=${IMAGE_PULL_PREFIX} \
--image_pull_secret=${IMAGE_PULL_SECRET} \
--upgrade=true \
--case_download=false
```

Monitor the status of pipeline pods
```
oc get pods | grep -E "portal-pipelines|wspipelines|pipeline-"
```

Monitor the status of the Pipelines custom resource

```
oc get WSPipelines wspipelines-cr -o yaml
```

IBM Orchestration Pipelines is upgraded when the apply-cr command returns
```
[SUCCESS]... The apply-cr command ran successfully
```

If you want to confirm that the custom resource status is Completed, you can run the cpd-cli manage get-cr-status command:
```
cpd-cli manage get-cr-status \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--components=ws_pipelines
```


### Upgrade DataStage custom resource (est. 1-2 minutes not including patching):

***Potential Issue During Previous DataStage Custom Resource Upgrade***

During the previous upgrade we encountered an issue in which the DataStage operator pod was crashing, for example:
```
oc get po -n $PROJECT_CPD_INST_OPERATORS | grep datastage-operator
```

```
ibm-cpd-datastage-operator-665cd9c4d8-9mxw9                       0/1     CrashLoopBackOff     8          77h
```

Upon closer inspection of the datastage operator pod, we discovered the pod was crashing due to an OOMKilled error:
```
oc describe pod ibm-cpd-datastage-operator-665cd9c4d8-9mxw9 -n $PROJECT_CPD_INST_OPERANDS
```

Under the status -> conditions section, look for any errors, such as OOMKilled

In order to address this, increase the datastage operator pod memory by updating the memory requests/limits in the CSV:
```
oc get csv -A | grep datastage
```

```
oc edit csv ibm-cpd-datastage-operator.v8.0.0
```

Increase the memory limit from 1Gi to 2Gi:
```
resources:
  limits:
	cpu: 500m
	memory: 2Gi

```

Wait a few minutes for the changes to be applied before proceeding with the upgrade

Proceed with the DataStage upgrade

Set the ${DATASTAGE_TYPE} variable to the edition of DataStage that you want to install:
```
export DATASTAGE_TYPE=datastage_ent_plus
```

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:
```
${CPDM_OC_LOGIN}
```

Update the custom resource for DataStage:
```
cpd-cli manage install-components \
--license_acceptance=true \
--components=datastage_ent_plus \
--release=${VERSION} \
--operator_ns=${PROJECT_CPD_INST_OPERATORS} \
--instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--image_pull_prefix=${IMAGE_PULL_PREFIX} \
--image_pull_secret=${IMAGE_PULL_SECRET} \
--upgrade=true \
--case_download=false
```

Upgrading DataStage also upgrades several dependencies in the following order
```
The install order for Level 0 is: ['platform-config'] -> should be completed when wkc/ccs was installed
The install order for Level 1 is: ['opencontent_opensearch'] -> should be completed when wkc/ccs was installed
The install order for Level 2 is: ['ccs'] -> should be completed when wkc/ccs was installed
The install order for Level 3 is: ['datastage_ent_plus']
```

Monitor the status of the DataStage custom resource:
```
oc get DataStage datastage -o yaml
```

DataStage is upgraded when the apply-cr command returns:

[SUCCESS]... The apply-cr command ran successfully

Note: The service instances are automatically upgraded when you upgrade DataStage

If you want to confirm that the custom resource status is Completed, you can run the cpd-cli manage get-cr-status command:
```
cpd-cli manage get-cr-status \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--components=datastage_ent_plus
```

DataStage is upgraded when the install-components command returns:

[SUCCESS]... The install-components command ran successfully


## 9. Upgrade the cpdbr service (est. 2 minutes):

Upgrade the cpdbr-tenant component for the instance

The command that you run depends on the type of storage that you use, where the cluster pulls images from, and whether the scheduling service is installed

For IBM Fusion environments without the scheduling service, run the following commands:
```
export CPDBR_OADP_IMAGE_VERSION=${VERSION}.amd64
export SPP_CPDBR_IMAGE_NAME=cpdbr-oadp:${CPDBR_OADP_IMAGE_VERSION}
cpd-cli oadp client config set namespace=ibm-backup-restore
cpd-cli oadp client config set cpd-namespace=cp4d
export OADP_PROJECT=ibm-backup-restore
export PROJECT_CPD_INST_OPERATORS=cp4d-operators
export PRIVATE_REGISTRY_LOCATION="nexus.lan.huk-coburg.de:2743"
```

```
cpd-cli oadp install \
--component=cpdbr-tenant \
--cpdbr-hooks-image-prefix=${PRIVATE_REGISTRY_LOCATION} \
--tenant-operator-namespace=${PROJECT_CPD_INST_OPERATORS} \
--cpdbr-hooks-image-prefix=${PRIVATE_REGISTRY_LOCATION}/cpopen/cpd \
--cpfs-image-prefix=${PRIVATE_REGISTRY_LOCATION}/cpopen/cpfs \
--upgrade=true \
--skip-recipes=true \
--log-level=debug \
--verbose
```

```
# regenerate the parent recipe "ibmcpd-tenant" 
cpd-cli oadp generate plan fusion parent-recipe --tenant-operator-namespace=${PROJECT_CPD_INST_OPERATORS} --verbose --log-level=debug

#velero ressourcen erhöhen
oc patch dpa -n ibm-backup-restore velero --type merge -p '{"spec": {"configuration": {"velero": {"podConfig": {"resourceAllocations": {"limits": {"ephemeral-storage": "2Gi", "memory": "4Gi"}}}}}}}'
```

## 10. Upgrade privileged monitors (est. 2 minutes)

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:
```
${CPDM_OC_LOGIN}
```

Upgrade the privileged monitors

The command that you run depends on whether you want the Operator namespace status check to include information about the scheduling service

To include only information about the operators project run the following command:
```
cpd-cli manage apply-privileged-monitoring-service \
--privileged_service_ns=${PROJECT_PRIVILEGED_MONITORING_SERVICE} \
--cpd_operator_ns=${PROJECT_CPD_INST_OPERATORS} \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS}
```


## 11. Upgrade the configuration admission controller webhook (est. 2 minutes):

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:
```
${CPDM_OC_LOGIN}
```

Upgrade the configuration admission controller webhook for x86_64 clusters:
```
cpd-cli manage install-cpd-config-ac \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--cpd_config_ac_image=${PRIVATE_REGISTRY_LOCATION}/cpopen/cpd/zen-rsi-adm-controller:${VERSION}.amd64 \
--case_download=false
```

Re-enable the configuration admission controller webhook:
```
cpd-cli manage enable-cpd-config-ac \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS}
```

## 12. Validate CPD upgrade (customer acceptance test):
End of document
