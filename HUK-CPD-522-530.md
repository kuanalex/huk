## HUK CPD upgrade 5.2.2 to 5.3.0

## Author: Alex Kuan (alex.kuan@ibm.com)

From:
```
CPD: 5.2.2
OCP: 4.18
Storage: Spectrum Scale 2.10.1
Internet: proxy
Private container registry: yes
Components: ibm-cert-manager,ibm-licensing,cpfs,cpd_platform,datastage_ent_plus,ws_pipelines,wkc,mantaflow
```

To:
```
CPD: 5.3.0
OCP: 4.18
Storage: Spectrum Scale 2.10.1
Internet: proxy
Private container registry: yes
Components: ibm-cert-manager,ibm-licensing,cpfs,cpd_platform,datastage_ent_plus,ws_pipelines,wkc,mantaflow
```

Upgrade flow and steps
```
1. CPD 5.2.2 pre-check
2. Update cpd-cli and environment variables script for 5.3.0
3. Backup CPD 5.2.2 CRs, cp4d and cp4d-operators namespaces
4. Revert any patches on the current installation
5. Upgrade shared cluster components (,ibm-licensing)
6. Prepare to upgrade an instance of IBM Software Hub
7. Upgrade an instance of IBM Software Hub
8. Upgrade CPD services (datastage_ent_plus,ws_pipelines,wkc,mantaflow)
9. Apply patches (CCS Hotfix and IKC Connectivity Hotfix, Orchestration Pipelines Hotfix, DataStage Patch 1)
10. Upgrade the cpdbr service
11. Upgrade privileged monitors
12. Validate CPD upgrade (customer acceptance test)
```


## 1. CPD 5.3.0 pre-check

Use a client workstation with internet (bastion or infra node) to download OCP and CPD images, and confirm the OS level, ensuring the OS is RHEL 8/9
```
cat /etc/redhat-release
```

Test internet connection, and make sure the output from the target URL and it can be connected successfully:
```
curl -v https://github.com/IBM
```

Prepare customer's IBM entitlement key

Log in to <https://myibm.ibm.com/products-services/containerlibrary> with the IBMid and password that are associated with the entitled software.

On the Get entitlement key tab, select Copy key to copy the entitlement key to the clipboard.

Save the API key in a text file.

Make sure free disk space more than 500 GB (to download images and pack the images into a tar ball)
```
df -lh
```

Collect OCP and CPD cluster information

Log into OCP cluster from bastion node
```
oc login $(oc whoami --show-server) -u kubeadmin -p <kubeadmin-password>
```

Review OCP version
```
oc get clusterversion
```

Review storage classes
```
oc get sc
```

Review OCP cluster status

Make sure all nodes are in ready status
```
oc get nodes
```

Make sure all mc are in correct status, UPDATED all True, UPDATING all False, DEGRADED all False
```
oc get mcp
```

Make sure all co are in correct status, AVAILABLE all True, PROGRESSING all False, DEGRADED all False
```
oc get co
```

Make sure there are no unhealthy pods, if there are, please open an IBM support case to fix them
```
oc get po -A -owide | egrep -v '([0-9])/\1' | egrep -v 'Completed' 
```

Get CPD installed projects
```
oc get pod -A | grep zen
```

Get CPD version and installed components
```
cpd-cli manage login-to-ocp \
--username=${OCP_USERNAME} \
--password=${OCP_PASSWORD} \
--server=${OCP_URL}
```

```
cpd-cli manage get-cr-status --cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS}
```

or
```
cpd-cli manage list-deployed-components --cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS}
```

Check the scheduling service, if it is installed but not in ibm-common-services project, uninstall it
```
oc get scheduling -A
```

Check install plan is automatic
```
oc get ip -A
```

Check the spec of each CPD custom resource, remove any patches before upgrading
```
oc project ${PROJECT_CPD_INST_OPERANDS}
```

```
for i in $(oc api-resources | grep cpd.ibm.com | awk '{print $1}'); do echo "************* $i *************"; for x in $(oc get $i --no-headers | awk '{print $1}'); do echo "--------- $x ------------"; oc get $i $x -o jsonpath={.spec} | jq; done ; done
```

Probe IBM registry (if required)
```
podman login cp.icr.io -u cp -p ${IBM_ENTITLEMENT_KEY}
```

If the IBM Certificate manager (ibm-cert-manager) is installed on your cluster, use the following steps to migrate your certificates from the IBM Certificate manager to the Red Hat OpenShift certificate manager (cert-manager Operator)
```
https://www.ibm.com/docs/en/software-hub/5.3.x?topic=upgrading-migrating-red-hat-openshift-certificate-manager
```

### Backing up your existing certificates

Before you uninstall the IBM Certificate manager, back up the Issuer and Certificate custom resources

Create a temporary project where you can validate that the IBM Certificate manager is working correctly
```
oc new-project cert-mgr-test
```

Back up the Issuer custom resources to a file named issuer_list.yaml
```
oc get issuers.cert-manager.io -A -o yaml > issuer_list.yaml
```

Back up the Certificate custom resources to a file named certificate_list.yaml
```
oc get certificates.cert-manager.io -A -o yaml > certificate_list.yaml
```

Verify that IBM Certificate manager is working correctly

Create an Issuer custom resource called verify-issuer
```
cat <<EOF |oc apply -f -
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: verify-issuer
spec:
  selfSigned: {}
EOF
```

Apply the contents of the issuer_list.yaml file
```
oc apply -f issuer_list.yaml
```

Create a Certificate custom resource called verify-certificate
```
cat <<EOF |oc apply -f -
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: verify-certificate
spec:
  commonName: verify-certificate
  secretName: verify-secret
  issuerRef:
    name: verify-issuer
    kind: Issuer
    group: cert-manager.io
EOF
```

Apply the contents of the certificate_list.yaml file
```
oc apply -f certificate_list.yaml
```

Verify that the verify-certificate custom resource is ready
```
oc get issuers.cert-manager.io
```

### Uninstall IBM Certificate manager 

Before you can install the Red Hat® OpenShift® certificate manager (cert-manager Operator), you must uninstall the IBM Certificate manager

Get the list of the IBM Certificate manager configuration instances
```
oc get certmanagerconfig \
-n ${PROJECT_CERT_MANAGER}
```

Delete each configuration instance returned by the preceding command
```
oc delete certmanagerconfig <name> \
-n ${PROJECT_CERT_MANAGER}
```

Uninstall the IBM Certificate manager operator

Delete the operator subscription:
```
oc delete sub ibm-cert-manager-operator \
-n ${PROJECT_CERT_MANAGER}
```

Find any ibm-cert-manager cluster service versions (CSVs)
```
oc get csv \
-n ${PROJECT_CERT_MANAGER} \
| grep ibm-cert-manager
```

Delete the CSVs returned by the previous command
```
oc delete csv <name> \
-n ${PROJECT_CERT_MANAGER}
```

Verify that the following IBM Certificate manager resources are deleted

Check for any deployments with the app.kubernetes.io/component=cert-manager label
```
oc get deployments \
-n ${PROJECT_CERT_MANAGER} \
-l app.kubernetes.io/component=cert-manager
```

If any deployments are returned by the preceding command, delete them
```
oc delete deployments <name> \
-n ${PROJECT_CERT_MANAGER}
```

Check for any services with the following app=ibm-cert-manager-webhook label
```
oc get service \
-n ${PROJECT_CERT_MANAGER} \
-l app=ibm-cert-manager-webhook
```

If any services are returned by the preceding command, delete them
```
oc delete service <name> \
-n ${PROJECT_CERT_MANAGER}
```

Check for any cert-manager-webhook mutating web hook configurations
```
oc get mutatingwebhookconfiguration \
-n ${PROJECT_CERT_MANAGER} \
| grep cert-manager-webhook
```

If any mutating web hook configurations are returned by the preceding command, delete them
```
oc delete mutatingwebhookconfiguration <name> \
-n ${PROJECT_CERT_MANAGER}
```

Check for any cert-manager-webhook validating web hook configurations
```
 oc get validatingwebhookconfiguration \
-n ${PROJECT_CERT_MANAGER} \
| grep cert-manager-webhook
```

If any validating web hook configurations are returned by the preceding command, delete them
```
oc delete validatingwebhookconfiguration <name> \
-n ${PROJECT_CERT_MANAGER}
```

### Mirroring Red Hat OpenShift certificate manager images to a private container registry

[If your cluster pulls images from a private container registry, you must mirror the Red Hat OpenShift certificate manager images to your private container registry before you install the certificate manager.](https://www.ibm.com/docs/en/software-hub/5.3.x?topic=manager-mirroring-red-hat-openshift-certificate-images)

### Installing the Red Hat OpenShift Container Platform cert-manager Operator

[You must install Red Hat OpenShift Container Platform cert-manager Operator before you upgrade to IBM Software Hub Version 5.3](https://www.ibm.com/docs/en/software-hub/5.3.x?topic=manager-installing-cert-operator)

Verify that the OLM subscription is created by running the following command
```
oc get subscription -n cert-manager-operator
```

Example output
```
NAME                              PACKAGE                           SOURCE             CHANNEL
openshift-cert-manager-operator   openshift-cert-manager-operator   redhat-operators   stable-v1
```

Verify whether the Operator is successfully installed by running the following command
```
oc get csv -n cert-manager-operator
```

Example output
```
NAME                            DISPLAY                                       VERSION   REPLACES                        PHASE
cert-manager-operator.v1.13.0   cert-manager Operator for Red Hat OpenShift   1.13.0    cert-manager-operator.v1.12.1   Succeeded
```

Verify that the status cert-manager Operator for Red Hat OpenShift is Running by running the following command
```
oc get pods -n cert-manager-operator
```

Example output
```
NAME                                                        READY   STATUS    RESTARTS   AGE
cert-manager-operator-controller-manager-695b4d46cb-r4hld   2/2     Running   0          7m4s
```

Verify that the status of cert-manager pods is Running by running the following command
```
oc get pods -n cert-manager
```

Example output
```
NAME                                       READY   STATUS    RESTARTS   AGE
cert-manager-58b7f649c4-dp6l4              1/1     Running   0          7m1s
cert-manager-cainjector-5565b8f897-gx25h   1/1     Running   0          7m37s
cert-manager-webhook-9bc98cbdd-f972x       1/1     Running   0          7m40s
```


## 2. Update cpd-cli, Helm CLI and environment variables script for 5.3.0

Download and unpack the latest cpd-cli release
```
wget https://github.com/IBM/cpd-cli/releases/download/v14.3.0_refresh_2/cpd-cli-linux-EE-14.3.0.tgz && gzip -d cpd-cli-linux-EE-14.3.0.tgz && tar -xvf cpd-cli-linux-EE-14.3.0.tar && rm -rf cpd-cli-linux-EE-14.3.0.tar
```

Add the cpd-cli to your PATH variable, for example
```
export PATH=/root/cpd-cli-linux-EE-14.3.0-2819:$PATH
```

Update your environment variables script
```
vi cpd_vars.sh
```

Update the Version field and save your changes
```
VERSION=5.3.0
```

Source your environment variables
```
source cpd_vars.sh
```

```
export OLM_UTILS_IMAGE=${PRIVATE_REGISTRY_LOCATION}/cpopen/cpd/olm-utils-v4@sha256:28eb9cbe0201ca5f520bd0b58a43206103e7a8c4b3be8b045db34e2840aa905f
```

Launch olm-utils-play-v4 container
```
cpd-cli manage restart-container
```

Download and install the latest release of Helm CLI
```
https://github.com/helm/helm/releases
```

Or run a wget command such as this one
```
wget https://get.helm.sh/helm-v4.0.4-linux-amd64.tar.gz && gzip -d helm-v4.0.4-linux-amd64.tar.gz  && tar -xvf helm-v4.0.4-linux-amd64.tar && rm -rf helm-v4.0.4-linux-amd64.tar
```

Replace old Helm CLI files in /usr/local/bin/helm on thet bastion node
```
mv linux-amd64/helm /usr/local/bin/helm
chmod +x /usr/local/bin/helm
```

Check the Helm CLI version
```
helm version
```

Output should look like this
```
version.BuildInfo{Version:"v4.0.4", GitCommit:"8650e1dad9e6ae38b41f60b712af9218a0d8cc11", GitTreeState:"clean", GoVersion:"go1.25.5", KubeClientVersion:"v1.34"}
```

If the scheduling service is installed on the cluster, you must update the cluster-scoped resources and create an image pull secret in the project where you plan to install the scheduling service
```
https://www.ibm.com/docs/en/software-hub/5.3.x?topic=pyc-updating-cluster-scoped-resources-shared-cluster-components
```

```
https://www.ibm.com/docs/en/software-hub/5.3.x?topic=pyc-creating-image-pull-secrets-shared-cluster-components-1
```


## 3. Backup CPD 5.2.2 CRs, cp4d and cp4d-operators namespaces

Create a new directory and store the output of the following commands in that directory
```
mkdir cpdbackup && cd cpdbackup && oc project ${PROJECT_CPD_INST_OPERANDS}
for i in $(oc api-resources | grep cpd.ibm.com | awk '{print $1}'); do echo "************* $i *************"; for x in $(oc get $i --no-headers | awk '{print $1}'); do echo "--------- $x ------------"; oc get $i $x -oyaml > bak-$x.yaml; done ; done
```

**Note: The following 'oc adm' commands can be time-consuming and should be collected during the pre-upgrade Health Check activity**

Backup the current state of operands in your backup folder of choice:
```
mkdir operandsbackup && cd operandsbackup && oc adm inspect -n ${PROJECT_CPD_INST_OPERANDS} --dest-dir=source-${PROJECT_CPD_INST_OPERANDS} $(oc api-resources --namespaced=true --verbs=get,list --no-headers -o name | tr '\n' ',' | sed 's/,$//')
```

Backup the current state of operators in your backup folder of choice:
```
mkdir operatorsbackup && cd operatorsbackup && oc adm inspect -n ${PROJECT_CPD_INST_OPERATORS} --dest-dir=source-${PROJECT_CPD_INST_OPERATORS} $(oc api-resources --namespaced=true --verbs=get,list --no-headers -o name | tr '\n' ',' | sed 's/,$//')
```


## 4. Revert Patches And Hot Fixes (skip for now...)


## 5. Upgrade shared cluster components (ibm-licensing, ibm-scheduling) (est. 3 minutes)

Determine which project the License Service is in
```
oc get deployment -A | grep ibm-licensing-operator
```

Upgrade the License Service

The License Service is in the ${PROJECT_LICENSE_SERVICE} project

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster
```
${CPDM_OC_LOGIN}
```

Upgrade shared cluster components
```
cpd-cli manage apply-cluster-components \
--release=${VERSION} \
--license_acceptance=true \
--licensing_ns=${PROJECT_LICENSE_SERVICE}
```

Then run the command apply-cluster-components command again

Wait for the cpd-cli to return the following message before proceeding to the next step

[SUCCESS] ... The apply-cluster-components command ran successfully

Confirm that the License Service pods are Running or Completed

```
oc get pods --namespace=${PROJECT_LICENSE_SERVICE}
```


## 6. Prepare to upgrade an instance of IBM Software Hub (est. 3 minutes)

### [Check the health of your cluster before proceeding with the upgrade](https://www.ibm.com/docs/en/software-hub/5.3.x?topic=puish-checking-health-your-cluster-1):
```
cpd-cli health operators \
--control_plane_ns=${PROJECT_CPD_INST_OPERANDS} \
--operator_ns=${PROJECT_CPD_INST_OPERATORS}
```

Results should read "SUCCESS..."
```
cpd-cli health operands \
--control_plane_ns=${PROJECT_CPD_INST_OPERANDS} \
--include_ns=${PROJECT_CPD_INST_OPERATORS}
```

Results should read "SUCCESS..."
```
cpd-cli health cluster
```

Results should read "SUCCESS..."
```
cpd-cli health nodes
```

Results should read "SUCCESS..."

### [Update the cluster-scoped resources for the platform and services](https://www.ibm.com/docs/en/software-hub/5.3.x?topic=puish-updating-cluster-scoped-resources-instance):

Generate the cluster_scoped_resources.yaml file. The command requires the CASE packages to be on the workstation (est. 3-4 minutes)
```
cpd-cli manage case-download \
--components=${COMPONENTS} \
--release=${VERSION} \
--operator_ns=${PROJECT_CPD_INST_OPERATORS} \
--cluster_resources=true
```

Change to the work directory
```
cd cpd-cli-workspace/olm-utils-workspace/work
```

Log in to Red Hat® OpenShift® Container Platform as a cluster administrator
```
${OC_LOGIN}
```

Apply the cluster-scoped resources for the from the cluster_scoped_resources.yaml file:
```
oc apply -f cluster_scoped_resources.yaml \
--server-side \
--force-conflicts
```

Optional: If you want a record of the resources that you generated, rename the cluster_scoped_resources.yaml
```
mv cluster_scoped_resources.yaml ${VERSION}-${PROJECT_CPD_INST_OPERATORS}-cluster_scoped_resources.yaml
```

### [Confirm whether the NamespaceScope operator is using the minimum RBAC, run the following command](https://www.ibm.com/docs/en/software-hub/5.3.x?topic=puish-reauthorizing-namespacescope-operator-minimum-rbac) (est. 1-2 minutes):

To confirm whether the NamespaceScope operator is using the minimum RBAC, run the following command:
```
oc get role nss-managed-role-from-${PROJECT_CPD_INST_OPERATORS} \
-n ${PROJECT_CPD_INST_OPERATORS} \
-o json | jq 'any(.rules[].apiGroups[]; . == "*")'
```

If the command returns TRUE, the operator does not need to be reauthorized, move on to the next step

If the command returns FALSE, the operator is using the minimum RBAC and must be reauthorized, follow the procedure here
```
https://www.ibm.com/docs/en/software-hub/5.3.x?topic=puish-reauthorizing-namespacescope-operator-minimum-rbac
```

### [Confirm if you need to reauthorize an instance administrator with the minimum RBAC to upgrade components](https://www.ibm.com/docs/en/software-hub/5.3.x?topic=puish-reauthorizing-user-minimum-rbac) (est. 1-2 minutes):

### [Apply entitlements](https://www.ibm.com/docs/en/software-hub/5.2.x?topic=puish-applying-your-entitlements-3) (est. 1-2 minutes):

Run the apply-entitlement command for each solution that is installed or that you plan to install in this instance

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:
```
${CPDM_OC_LOGIN}
```

Apply entitlements for CP4D (includes WKC and Mantaflow):

NON-PR and WA
```
cpd-cli manage apply-entitlement \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--entitlement=cpd-enterprise \
--production=false
```

PR
```
cpd-cli manage apply-entitlement \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--entitlement=cpd-enterprise
```

Apply entitlements for DataStage:

```
export LICENSE_NAME=datastage-plus
```

NON-PR and WA
```
cpd-cli manage apply-entitlement \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--entitlement=${LICENSE_NAME} \
--production=false
```

PR
```
cpd-cli manage apply-entitlement \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--entitlement=${LICENSE_NAME}
```


## 7. Upgrade an instance of IBM Software Hub (est. 30 minutes)

### Create image pull secrets for an instance of IBM Software Hub:

You must create an image pull secret in the projects that are associated with an instance of IBM Software Hub

The image pull secrets ensure that any components that use Helm have access to the credentials for pulling images

Log in to Red Hat® OpenShift® Container Platform as a user with sufficient permissions to complete the task.
```
${OC_LOGIN}
```

Set your $IMAGE_PULL_SECRET and $IMAGE_PULL_CREDENTIALS environment variables
```
export IMAGE_PULL_SECRET=<pull-secret-name>
export IMAGE_PULL_CREDENTIALS=$(echo -n "$PRIVATE_REGISTRY_PULL_USER:$PRIVATE_REGISTRY_PULL_PASSWORD" | base64 -w 0)
export IMAGE_PULL_PREFIX=${PRIVATE_REGISTRY_LOCATION}
```

Create a file named dockerconfig.json based on where your cluster pulls images from

For private container registry
```
cat <<EOF > dockerconfig.json 
{
  "auths": {
    "PRIVATE_REGISTRY_LOCATION": {
      "auth": "${IMAGE_PULL_CREDENTIALS}"
    }
  }
}
EOF
```

Create the image pull secret in the operators project for the instance:
```
oc create secret docker-registry ${IMAGE_PULL_SECRET} \
--from-file ".dockerconfigjson=dockerconfig.json" \
--namespace=${PROJECT_CPD_INST_OPERATORS}
```

Create the image pull secret in the operands project for the instance:
```
oc create secret docker-registry ${IMAGE_PULL_SECRET} \
--from-file ".dockerconfigjson=dockerconfig.json" \
--namespace=${PROJECT_CPD_INST_OPERANDS}
```

If your instance includes tethered projects, create the image pull secret in each project that you plan to tether to the operands project:
```
oc create secret docker-registry ${IMAGE_PULL_SECRET} \
--from-file ".dockerconfigjson=dockerconfig.json" \
--namespace=${PROJECT_CPD_INSTANCE_TETHERED}
```

Repeat this step for each tethered project that is associated with this instance of IBM Software Hub

Now that you've created the image pull secrets for this instance of IBM Software Hub, you're ready to upgrade IBM Software Hub

### [Upgrade the required operators and custom resources for the instance](https://www.ibm.com/docs/en/software-hub/5.3.x?topic=uish-upgrading-software-hub):

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:
```
${CPDM_OC_LOGIN}
```

Review the license terms for the software that you plan to install:
```
cpd-cli manage get-license \
--release=${VERSION} \
--license-type=EE
```

Upgrade the required operators and custom resources for the instance (est. 36 minutes):
```
cpd-cli manage install-components \
--license_acceptance=true \
--components=cpd_platform \
--release=${VERSION} \
--operator_ns=${PROJECT_CPD_INST_OPERATORS} \
--instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--image_pull_prefix=${IMAGE_PULL_PREFIX} \
--image_pull_secret=${IMAGE_PULL_SECRET} \
--run_storage_tests=false \
--upgrade=true
```

Monitor the upgrade progress of the custom resources with the following commands:
```
oc get ZenService lite-cr -n cp4d -o yaml
```

```
oc get Ibmcpd ibmcpd-cr -n cp4d -o yaml
```

Wait for the cpd-cli to return the following message before proceeding to the next step:

[SUCCESS]... The install-components command ran successfully


## 8. Upgrade CPD services (datastage_ent_plus,ws_pipelines,wkc,mantaflow)

### Upgrade CCS custom resource (est. 30 minutes):
```
cpd-cli manage install-components \
--license_acceptance=true \
--components=ccs \
--release=${VERSION} \
--operator_ns=${PROJECT_CPD_INST_OPERATORS} \
--instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--image_pull_prefix=${IMAGE_PULL_PREFIX} \
--image_pull_secret=${IMAGE_PULL_SECRET} \
--upgrade=true
```

Monitor ccs pods for any errors
```
oc get pods -n cp4d | grep -E 'ccs|runtime'
```

Monitor ccs-cr yaml for progress
```
oc get ccs ccs-cr -o yaml
```

```
oc get ccs ccs-cr -o yaml | grep progress
```

After you've confirmed CCS is fully upgraded, you can proceed with the upgrade of WKC


### Upgrade WKC custom resource (est. 40 minutes not including patching):

When you upgrade IBM Knowledge Catalog, the options that you specified when you installed IBM Knowledge Catalog are used

Ensure you have specified the correct installation options in the wkc-install-options.yml in the cpd-cli work directory (For example: /data/cpdcli/cpd-cli-workspace/olm-utils-workspace/work)

Change directories to your cpd-cli-workspace/olm-utils-workspace/work directory
```
cd cpd-cli-workspace/olm-utils-workspace/work/
```

Create the wkc-install-options.yml for IBM Knowledge Catalog - The following values were extracted from the latest revision of HUK internal runbook:
```
################################################################################
# IBM Knowledge Catalog parameters
################################################################################
custom_spec:
  wkc:
    enableDataQuality: True
    enableSemanticAutomation: False
    enableKnowledgeGraph: True
    useFDB: True
```

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster
```
${CPDM_OC_LOGIN}
```

Set the IKC_TYPE environment variable
```
export IKC_TYPE=wkc
```

Update the custom resource for IBM Knowledge Catalog
```
cpd-cli manage install-components \
--license_acceptance=true \
--components=${IKC_TYPE} \
--release=${VERSION} \
--operator_ns=${PROJECT_CPD_INST_OPERATORS} \
--instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--image_pull_prefix=${IMAGE_PULL_PREFIX} \
--image_pull_secret=${IMAGE_PULL_SECRET} \
--param-file=/tmp/work/wkc-install-options.yml \
--upgrade=true
```

Upgrading wkc also upgrades several dependencies in the following order
```
The install order for Level 0 is: ['platform-config']
The install order for Level 1 is: ['opencontent_opensearch']
The install order for Level 2 is: ['ccs'] (should already be installed from the previous step)
The install order for Level 3 is: ['datarefinery', 'opencontent_fdb', 'datastage_ent', 'analyticsengine', 'ibm_neo4j']
The install order for Level 4 is: ['wkc']
```

IBM Knowledge Catalog is upgraded when the apply-cr command returns:

[SUCCESS]... The apply-cr command ran successfully

If you want to confirm that the custom resource status is Completed, you can run the cpd-cli manage get-cr-status command:
```
cpd-cli manage get-cr-status \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--components=${IKC_TYPE}
```

Monitor for relevant pods, look for any pods in error statuses
```
oc get pods -n ${PROJECT_CPD_INST_OPERANDS} | grep -E "wkc-|wdp-|catalog-|governance-|dc-main|iis-|finley-|dq-"
```

```
oc get po -n cp4d | grep -E 'wkc|wdp' | grep -E 'Error|Crash|BackOff'
```

Monitor the status of the WKC custom resource for 'Completed':
```
oc get WKC wkc-cr -o yaml
```


### Upgrade MANTA Automated Data Lineage custom resource (est. 6-7 minutes):

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:
```
${CPDM_OC_LOGIN}
```

Update the custom resource for MANTA Automated Data Lineage:
```
cpd-cli manage install-components \
--license_acceptance=true \
--components=mantaflow \
--release=${VERSION} \
--operator_ns=${PROJECT_CPD_INST_OPERATORS} \
--instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--block_storage_class=${STG_CLASS_BLOCK} \
--file_storage_class=${STG_CLASS_FILE} \
--image_pull_prefix=${IMAGE_PULL_PREFIX} \
--image_pull_secret=${IMAGE_PULL_SECRET}
```

Monitor relevant mantaflow pods
```
oc get po | grep manta
```

Monitor the status of the MANTA custom resource:
```
oc get MantaFlow mantaflow-wkc -o yaml
```

MANTA Automated Data Lineage is upgraded when the apply-cr command returns:

[SUCCESS]... The apply-cr command ran successfully

If you want to confirm that the custom resource status is Completed, you can run the cpd-cli manage get-cr-status command:
```
cpd-cli manage get-cr-status \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--components=mantaflow
```

Mantaflow is upgraded when the install-components command returns:

[SUCCESS]... The install-components command ran successfully


### Upgrade IBM Orchestration Pipelines custom resource (est. 12 minutes not including patching):

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:
```
${CPDM_OC_LOGIN}
```

Update the custom resource for IBM Orchestration Pipelines:
```
cpd-cli manage install-components \
--license_acceptance=true \
--components=ws_pipelines \
--release=${VERSION} \
--operator_ns=${PROJECT_CPD_INST_OPERATORS} \
--instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--image_pull_prefix=${IMAGE_PULL_PREFIX} \
--image_pull_secret=${IMAGE_PULL_SECRET} \
--upgrade=true
```

Monitor the status of pipeline pods
```
oc get pods | grep -E "portal-pipelines|wspipelines|pipeline-"
```

Monitor the status of the Pipelines custom resource
```
oc get WSPipelines wspipelines-cr -o yaml
```

IBM Orchestration Pipelines is upgraded when the apply-cr command returns
```
[SUCCESS]... The apply-cr command ran successfully
```

If you want to confirm that the custom resource status is Completed, you can run the cpd-cli manage get-cr-status command:
```
cpd-cli manage get-cr-status \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--components=ws_pipelines
```


### Upgrade DataStage custom resource (est. 1-2 minutes not including patching):

Set the ${DATASTAGE_TYPE} variable to the edition of DataStage that you want to install:
```
export DATASTAGE_TYPE=datastage_ent_plus
```

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:
```
${CPDM_OC_LOGIN}
```

Update the custom resource for DataStage:
```
cpd-cli manage install-components \
--license_acceptance=true \
--components=datastage_ent_plus \
--release=${VERSION} \
--operator_ns=${PROJECT_CPD_INST_OPERATORS} \
--instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--image_pull_prefix=${IMAGE_PULL_PREFIX} \
--image_pull_secret=${IMAGE_PULL_SECRET} \
--upgrade=true
```

Upgrading DataStage also upgrades several dependencies in the following order
```
The install order for Level 0 is: ['platform-config'] -> should be completed when wkc/ccs was installed
The install order for Level 1 is: ['opencontent_opensearch'] -> should be completed when wkc/ccs was installed
The install order for Level 2 is: ['ccs'] -> should be completed when wkc/ccs was installed
The install order for Level 3 is: ['datastage_ent_plus']
```

Monitor the status of the DataStage custom resource:
```
oc get DataStage datastage -o yaml
```

Note: The service instances are automatically upgraded when you upgrade DataStage

If you want to confirm that the custom resource status is Completed, you can run the cpd-cli manage get-cr-status command:
```
cpd-cli manage get-cr-status \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--components=datastage_ent_plus
```

DataStage is upgraded when the install-components command returns:

[SUCCESS]... The install-components command ran successfully

## 9. Apply patches (CCS Hotfix and IKC Connectivity Hotfix, Orchestration Pipelines Hotfix, DataStage Patch 1):

### Apply the [IBM Knowledge Catalog 5.3.0 Patch](https://www.ibm.com/support/pages/node/7254566#online-hotfix)

The following instructions assume that all relevant images have been mirrored prior to proceeding
```
export PROJECT_CPD_INST_OPERANDS=cp4d
```

For CCS, save a copy of the Common Core Services CR currently deployed on the cluster
```
oc get ccs ccs-cr -o yaml -n ${PROJECT_CPD_INST_OPERANDS} > ccs_cr_backup_<date>.yaml
```

Where <date> is the date you are applying the patch

Run the following command to apply the patch to Common Core Services CR custom resource (ccs-cr)

For x86 installations
```
oc patch ccs ccs-cr -n ${PROJECT_CPD_INST_OPERANDS} --type=merge -p '{"spec":{"image_digests":{
    "wdp_connect_connection_image": "sha256:ba848283466f8f662cfdcd21047e3c6f4e103e5b70604614e92e6ff9bd2a17d7",
    "wdp_connect_connector_image": "sha256:b36e7298a46baa790ad294c33804b09d9404a2859b4960751dfc3438764d4a29",
    "wdp_connect_flight_image": "sha256:d24360c12eeca2a92a0ed66765d08dcfed78c63aeff4191298368a0851cfd144"
}}}'
```

Wait for the CCS operator reconciliation to complete. Run the following command to monitor the reconciliation status
```
oc get ccs ccs-cr -n ${PROJECT_CPD_INST_OPERANDS}
```

After a period of time, the patched pods in ${PROJECT_CPD_INST_OPERANDS} should be up and running with the updated images

For IKC, view the helm chart update history and there should be a new revision from the previous upgrade command
```
helm history -n ${PROJECT_CPD_INST_OPERANDS} wkc
```

NOTE: Keep track of the previous revision number in case the hotfix needs to be reverted
```
REVISION	UPDATED                 	STATUS    	CHART                        	APP VERSION	DESCRIPTION     
1       	Thu Jan  8 17:29:29 2026	superseded	wkc-migration-0.0.0          	0.0.0      	Install complete
2       	Thu Jan  8 17:29:52 2026	deployed  	wkc-5.3.0+20251203.234930.305	5.3.0      	Upgrade complete
```

Copy the Version 5.3.0 helm chart (ibm-wkc-5.3.0+20251203.234930.305.tgz) to a local location and run the following commands to apply the IKC operator hotfix image override to the IKC helm chart
```
oc label wkc wkc-cr -n ${PROJECT_CPD_INST_OPERANDS} app.kubernetes.io/managed-by=Helm component-id=wkc icpdsupport/addOnId=wkc  
oc annotate wkc wkc-cr -n ${PROJECT_CPD_INST_OPERANDS}  meta.helm.sh/release-name=wkc meta.helm.sh/release-namespace=${PROJECT_CPD_INST_OPERANDS}
```

For x86 installations
```
helm upgrade wkc cpd-cli-workspace/olm-utils-workspace/work/offline/5.3.0/.ibm-pak/data/cases/ibm-wkc/5.3.0/charts/wkc-5.3.0+20251203.234930.305.tgz --reuse-values --set wkc.operatorImageDigest.amd64=sha256:7b601d78a630904909d310ba46d5bdf64930e059197483f58a7ad68c7292dc4e
```

The helm chart may be downloaded from [here](https://github.com/IBM/charts/blob/master/repo/ibm-helm/wkc-5.3.0%2B20251203.234930.305.tgz) or copied from the local cpd-cli workspace directory

View the helm chart update history and there should be a new revision from the previous upgrade command
```
helm history -n ${PROJECT_CPD_INST_OPERANDS} wkc
```

NOTE: Keep track of the previous revision number in case the hotfix needs to be reverted
```
REVISION	UPDATED                 	STATUS    	CHART                        	APP VERSION	DESCRIPTION     
1       	Thu Jan  8 17:29:29 2026	superseded	wkc-migration-0.0.0          	0.0.0      	Install complete
2       	Thu Jan  8 17:29:52 2026	superseded	wkc-5.3.0+20251203.234930.305	5.3.0      	Upgrade complete
3       	Thu Jan  8 13:53:11 2026	deployed  	wkc-5.3.0+20251203.234930.305	5.3.0      	Upgrade complete
```

Wait for the WKC operator reconciliation to complete. Run the following command to monitor the reconciliation status
```
oc get wkc wkc-cr -n ${PROJECT_CPD_INST_OPERANDS}
```

Monitor for relevant pods, look for any pods in error statuses
```
oc get po -n cp4d | grep -E 'wkc|wdp' | grep -E 'Error|Crash|BackOff'
```

After a period of time, the patched pods in ${PROJECT_CPD_INST_OPERANDS} should be up and running with the updated images

### Apply Orchestration Pipelines Hotfix

The following instructions assume that all relevant images have been mirrored prior to proceeding

Create a backup of existing CRD resource
```
oc get crd wspipelines.wspipelines.cpd.ibm.com -o yaml > wspipelines-crd-bck.yaml
```

Patch existing CRD resource
```
oc patch crd wspipelines.wspipelines.cpd.ibm.com --type='json' -p='[{"op": "add", "path": "/spec/versions/0/schema/openAPIV3Schema/properties/spec/properties/enableTransientPVC", "value": {"type": "boolean"}}]'
```

Verify CRD patch step
```
oc get crd wspipelines.wspipelines.cpd.ibm.com -o jsonpath='{.spec.versions[0].schema.openAPIV3Schema.properties.spec.properties.enableTransientPVC}'
```
Output should be
```
{"type":"boolean"}
```

Pull Orchestration Pipelines 5.3.0 GA helm chart:
```
curl -L -o ws-pipelines-12.0.0+20251208.115837.94.tgz \
'https://github.com/IBM/charts/raw/refs/heads/master/repo/ibm-helm/ws-pipelines-12.0.0+20251208.115837.94.tgz'
```

Verify existing helm release
```
helm list -A | grep ws-pipelines
```

Set helm release namespace
```
export HELM_NAMESPACE="cp4d"
export PROJECT_CPD_INSTANCE=cp4d
export PROJECT_CPD_OPERATORS=cp4d-operators

```

Upgrade existing helm release with new hotfix image digests
```
helm upgrade ws-pipelines ws-pipelines-12.0.0+20251208.115837.94.tgz \
  --namespace $HELM_NAMESPACE \
  --reset-then-reuse-values \
  --set wsPipelines.operatorImageDigest.amd64="sha256:570a2cf3633ef6181b90e88eb34f47bdecc58659e618f52897b4b7194567f56b" 
```

Verify progress of reconciliation for CR
```
oc get wspipelines.wspipelines.cpd.ibm.com wspipelines-cr -n $PROJECT_CPD_INSTANCE
```

Wait until CR will be in completed status:
```
NAME             VERSION   RECONCILED   STATUS      PERCENT   AGE
wspipelines-cr   5.3.0     5.3.0        Completed   100%      73m
```


### Apply DataStage Patch 1

The following instructions assume that all relevant images have been mirrored prior to proceeding

Verify there are no additional digests specified in the datastage custom resource
```
oc edit datastage datastage -n ${PROJECT_CPD_INST_OPERANDS}
```

Look for a section labelled image_digests in the spec: section

If any images are present please remove them and wait for the reconciliation to complete before proceeding

For example, if your DataStage CR looks like this
```
spec:
  blockStorageClass: nfs-client
  ds_enterprise: false
  enableDataService: true
  fileStorageClass: nfs-client
  image_digests:
    canvas: sha256:2b28f90c5d2985c396281db0952de776787fd7868dae1b1ac42778a5a4312099
  license:
    accept: true
```

Modify it to look like this
```
spec:
  blockStorageClass: nfs-client
  ds_enterprise: false
  enableDataService: true
  fileStorageClass: nfs-client
  license:
    accept: true
```

Wait for the DataStage operator reconciliation to complete. Run the following command to monitor the reconciliation status
```
oc get datastage datastage -n ${PROJECT_CPD_INST_OPERANDS} 
```

After some time, the respective datastage pods in ${PROJECT_CPD_INST_OPERANDS} namespace should be up and running with the updated image

Note that some images will not update based on if the previous patches were applied

Find the pxruntime instance names
```
oc get pxruntime -n ${PROJECT_CPD_INST_OPERANDS}
```

Repeat this process and verify there are no additional digests specified in the px-runtime custom resource
```
oc edit pxruntime [px-runtime-instance] -n ${PROJECT_CPD_INST_OPERANDS}
```

Wait for the DataStage operator reconciliation to complete. Run the following command to monitor the reconciliation status:
```
oc get pxruntime [instance-name] -n ${PROJECT_CPD_INST_OPERANDS} 
```

After some time, the datastage px-runtime and px-compute pods in ${PROJECT_CPD_INSTANCE} will be reverted back to the default state

Download the latest DataStage helm bundle using cpd-cli

For DataStage Enterprise Plus use
```
cpd-cli manage case-download --release=5.3.0 --components=datastage_ent
```

Create a values-override.yaml file
```
cat > values-override.yaml << EOF
global:
  licenseAccept: true
  releaseVersion: 5.3.0
  operatorNamespace: ${PROJECT_CPD_INST_OPERATORS}
  instanceNamespace: ${PROJECT_CPD_INST_OPERANDS}
  tetheredNamespaces: []
  blockStorageClass: ibm-spectrum-scale-sc
  fileStorageClass: ibm-spectrum-scale-sc
  imagePullPrefix: icr.io
  imagePullSecrets: <your ibm entitlement key>
datastageEntPlus:
  deployCR: false
  operatorImageDigest:
    amd64: "sha256:cc693b970a62b45739b64248a0704a22aa0601e985dcca0359d75cdc9bc26452"
datastageEnt:
  deployCR: false
  operatorImageDigest:
    amd64: "sha256:cc693b970a62b45739b64248a0704a22aa0601e985dcca0359d75cdc9bc26452"
EOF
```

Now, run the helm upgrade command to apply the new patched operator image to the cluster

For DataStage Enterprise Plus, use this
```
helm upgrade datastage-ent <YOUR CPD-CLI WORKSPACE>/olm-utils-workspace/work/offline/5.3.0/.ibm-pak/data/cases/ibm-datastage-enterprise/11.0.0/charts/datastage-ent-11.0.0+20251203.100137.3359.tgz -f values-override.yaml
```

**Potential Issue during DataStage and Pxruntime patching**

If the DataStage operator is crashing/restarting, increase the operator memory via the deployment
```
oc edit deploy ibm-cpd-datastage-operator -n cp4d-operators
```

```
resources:
  limits:
	cpu: 500m
	memory: 2Gi
```

Wait for datastage and pxruntime custom resources to reconcile to ‘Completed’ state:
```
oc get datastage datastage -n ${PROJECT_CPD_INST_OPERANDS}
```
```
oc get pxruntime -n ${PROJECT_CPD_INST_OPERANDS}
```


## 10. Upgrade the cpdbr service (est. 2 minutes):

Upgrade the cpdbr-tenant component for the instance

The command that you run depends on the type of storage that you use, where the cluster pulls images from, and whether the scheduling service is installed

For IBM Fusion environments without the scheduling service, run the following commands:
```
export CPDBR_OADP_IMAGE_VERSION=${VERSION}.amd64
export SPP_CPDBR_IMAGE_NAME=cpdbr-oadp:${CPDBR_OADP_IMAGE_VERSION}
cpd-cli oadp client config set namespace=ibm-backup-restore
cpd-cli oadp client config set cpd-namespace=cp4d
export OADP_PROJECT=ibm-backup-restore
export PROJECT_CPD_INST_OPERATORS=cp4d-operators
export PRIVATE_REGISTRY_LOCATION="nexus.lan.huk-coburg.de:2743"
```

```
cpd-cli oadp install \
--component=cpdbr-tenant \
--cpdbr-hooks-image-prefix=${PRIVATE_REGISTRY_LOCATION} \
--tenant-operator-namespace=${PROJECT_CPD_INST_OPERATORS} \
--cpdbr-hooks-image-prefix=${PRIVATE_REGISTRY_LOCATION}/cpopen/cpd \
--cpfs-image-prefix=${PRIVATE_REGISTRY_LOCATION}/cpopen/cpfs \
--upgrade=true \
--skip-recipes=true \
--log-level=debug \
--verbose
```

```
# regenerate the parent recipe "ibmcpd-tenant" 
cpd-cli oadp generate plan fusion parent-recipe --tenant-operator-namespace=${PROJECT_CPD_INST_OPERATORS} --verbose --log-level=debug

#velero ressourcen erhöhen
oc patch dpa -n ibm-backup-restore velero --type merge -p '{"spec": {"configuration": {"velero": {"podConfig": {"resourceAllocations": {"limits": {"ephemeral-storage": "2Gi", "memory": "4Gi"}}}}}}}'
```

## 11. Upgrade privileged monitors (est. 2 minutes)

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:
```
${CPDM_OC_LOGIN}
```

Upgrade the privileged monitors

The command that you run depends on whether you want the Operator namespace status check to include information about the scheduling service

To include only information about the operators project run the following command:
```
cpd-cli manage apply-privileged-monitoring-service \
--privileged_service_ns=${PROJECT_PRIVILEGED_MONITORING_SERVICE} \
--cpd_operator_ns=${PROJECT_CPD_INST_OPERATORS} \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS}
```

**Potential Issue during the upgrade of privileged monitors**
```
The expected CatalogSource ibm-licensing-catalog from ibm-licensing CatalogSourceNamespace is not available for ibm-licensing-operator-app in ibm-licensing namespace. Also there are multiple CatalogSources for ibm-licensing-operator-app available in the cluster, please create the expected CatalogSource, or specify the correct CatalogSource name and namespace and re-run the script again.
```

The workaround for this issue is to delete the certificates the error complains about
```
oc delete issuer zen-watchdog-ops-service-ss-issuer -n ibm-cpd-privileged
oc delete certificate zen-watchdog-ops-service-ca-certificate -n ibm-cpd-privileged
oc delete issuer zen-watchdog-ops-service-tls-issuer -n ibm-cpd-privileged
oc delete certificate zen-watchdog-ops-internal-tls-certificate -n ibm-cpd-privileged
```


## 12. Validate CPD upgrade (customer acceptance test):
End of document



