## HUK CPD upgrade 5.2.1 to 5.2.2

## Author: Alex Kuan (alex.kuan@ibm.com)

From:

```
CPD: 5.2.1
OCP: 4.18
Storage: Spectrum Scale 2.10.1
Internet: proxy
Private container registry: yes
Components: ibm-cert-manager,ibm-licensing,cpfs,cpd_platform,datastage_ent_plus,ws_pipelines,wkc,mantaflow
```

To:

```
CPD: 5.2.2
OCP: 4.18
Storage: Spectrum Scale 2.10.1
Internet: proxy
Private container registry: yes
Components: ibm-cert-manager,ibm-licensing,cpfs,cpd_platform,datastage_ent_plus,ws_pipelines,wkc,mantaflow
```

Upgrade flow and steps

```
1. CPD 5.2.1 pre-check
2. Update cpd-cli and environment variables script for 5.2.2
3. Backup CPD 5.2.1 CRs, cp4d and cp4d-operators namespaces
4. Revert any patches on the current installation
5. Upgrade shared cluster components (ibm-cert-manager,ibm-licensing)
6. Prepare to upgrade an instance of IBM Software Hub
7. Upgrade an instance of IBM Software Hub
8. Upgrade CPD services (datastage_ent_plus,ws_pipelines,wkc,mantaflow)
9. Upgrade the cpdbr service
10. Upgrade privileged monitors
11. Upgrade the configuration admission controller webhook
12. Validate CPD upgrade (customer acceptance test)
```


## 1. CPD 5.2.1 pre-check

Use a client workstation with internet (bastion or infra node) to download OCP and CPD images, and confirm the OS level, ensuring the OS is RHEL 8/9

```
cat /etc/redhat-release
```

Test internet connection, and make sure the output from the target URL and it can be connected successfully:

```
curl -v https://github.com/IBM
```

Prepare customer's IBM entitlement key

Log in to <https://myibm.ibm.com/products-services/containerlibrary> with the IBMid and password that are associated with the entitled software.

On the Get entitlement key tab, select Copy key to copy the entitlement key to the clipboard.

Save the API key in a text file.

Make sure free disk space more than 500 GB (to download images and pack the images into a tar ball)

```
df -lh
```

Collect OCP and CPD cluster information

Log into OCP cluster from bastion node

```
oc login $(oc whoami --show-server) -u kubeadmin -p <kubeadmin-password>
```

Review OCP version

```
oc get clusterversion
```

Review storage classes

```
oc get sc
```

Review OCP cluster status

Make sure all nodes are in ready status

```
oc get nodes
```

Make sure all mc are in correct status, UPDATED all True, UPDATING all False, DEGRADED all False

```
oc get mcp
```

Make sure all co are in correct status, AVAILABLE all True, PROGRESSING all False, DEGRADED all False

```
oc get co
```

Make sure there are no unhealthy pods, if there are, please open an IBM support case to fix them.

```
oc get po -A -owide | egrep -v '([0-9])/\1' | egrep -v 'Completed' 
```

Get CPD installed projects

```
oc get pod -A | grep zen
```

Get CPD version and installed components

```
cpd-cli manage login-to-ocp \
--username=${OCP_USERNAME} \
--password=${OCP_PASSWORD} \
--server=${OCP_URL}
```

```
cpd-cli manage get-cr-status --cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS}
```

or

```
cpd-cli manage list-deployed-components --cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS}
```

Check the scheduling service, if it is installed but not in ibm-common-services project, uninstall it

```
oc get scheduling -A
```

Check install plan is automatic

```
oc get ip -A
```

Check the spec of each CPD custom resource, remove any patches before upgrading

```
oc project ${PROJECT_CPD_INST_OPERANDS}
```

```
for i in $(oc api-resources | grep cpd.ibm.com | awk '{print $1}'); do echo "************* $i *************"; for x in $(oc get $i --no-headers | awk '{print $1}'); do echo "--------- $x ------------"; oc get $i $x -o jsonpath={.spec} | jq; done ; done
```

Probe IBM registry (if required)

```
podman login cp.icr.io -u cp -p ${IBM_ENTITLEMENT_KEY}
```


## 2. Update cpd-cli and environment variables script for 5.2.2

Download and unpack the latest cpd-cli release

```
wget https://github.com/IBM/cpd-cli/releases/download/v14.2.2/cpd-cli-linux-EE-14.2.2.tgz && gzip -d cpd-cli-linux-EE-14.2.2.tgz && tar -xvf cpd-cli-linux-EE-14.2.2.tar && rm -rf cpd-cli-linux-EE-14.2.2.tar
```

Add the cpd-cli to your PATH variable, for example

```
export PATH=/root/cpd-cli-linux-EE-14.2.2-2727:$PATH
```

Update your environment variables script

```
vi cpd_vars.sh
```

Update the Version field and save your changes

```
VERSION=5.2.2
```

Source your environment variables

```
source cpd_vars.sh
```

Launch olm-utils-play-v3 container

```
cpd-cli manage restart-container
```


## 3. Backup CPD 5.2.1 CRs, cp4d and cp4d-operators namespaces

Create a new directory and store the output of the following commands in that directory

```
mkdir cpdbackup && cd cpdbackup && oc project ${PROJECT_CPD_INST_OPERANDS}
for i in $(oc api-resources | grep cpd.ibm.com | awk '{print $1}'); do echo "************* $i *************"; for x in $(oc get $i --no-headers | awk '{print $1}'); do echo "--------- $x ------------"; oc get $i $x -oyaml > bak-$x.yaml; done ; done
```

**Note: The following 'oc adm' commands can be time-consuming and should be collected during the pre-upgrade Health Check activity**

Backup the current state of operands in your backup folder of choice:

```
mkdir operandsbackup && cd operandsbackup && oc adm inspect -n ${PROJECT_CPD_INST_OPERANDS} --dest-dir=source-${PROJECT_CPD_INST_OPERANDS} $(oc api-resources --namespaced=true --verbs=get,list --no-headers -o name | tr '\n' ',' | sed 's/,$//')
```

Backup the current state of operators in your backup folder of choice:

```
mkdir operatorsbackup && cd operatorsbackup && oc adm inspect -n ${PROJECT_CPD_INST_OPERATORS} --dest-dir=source-${PROJECT_CPD_INST_OPERATORS} $(oc api-resources --namespaced=true --verbs=get,list --no-headers -o name | tr '\n' ',' | sed 's/,$//')
```

## 4. Revert Patches And Hot Fixes

It is recommended to remove patches or hot fixes before upgrading Cloud Pak for Data to avoid potential conflicts, inconsistencies, or compatibility issues that may arise during the upgrade process. 

Patches and hot fixes are typically applied to address specific issues or vulnerabilities, but they might not be fully compatible with the new CP4D version. 

Removing them before upgrading ensures a cleaner and more predictable upgrade experience, reducing the risk of unexpected behavior or errors. 

The current list of patches that need to be reverted/removed are the following:
- Datastage 5.2.1 Patch 4
- IKC 5.2.1 Connectivity Cumulative Patch
- Pipelines 5.2.1 Hot Fix 1

### Remove Datastage 5.2.1 Patch 4 (manual removal)

Set up your environment variables:

```
export PROJECT_CPD_INST_OPERATORS=cp4d-operators
export PROJECT_CPD_INST_OPERANDS=cp4d
```

Revert the datastage catalog source to the original one backed up from Step 2b:

```
oc patch catsrc -n ${PROJECT_CPD_INST_OPERATORS} ibm-cpd-datastage-ent-plus-operator-catalog --type='json' -p='[{"op": "replace", "path": "/spec/image", "value":"icr.io/cpopen/ds-operator-catalog@sha256:5de312187d22e501f8f15230195e949330749a1fcd7ee44ca1b9fd759152837f"}]'
```

Verify the DataStage catalog source pod is up and running:

```
oc get pod -n ${PROJECT_CPD_INST_OPERATORS} | grep ibm-cpd-datastage | grep catalog
```

Delete datastage subscription and CSV:

```
oc delete sub -n ${PROJECT_CPD_INST_OPERATORS} ibm-cpd-datastage-operator

oc delete csv -n ${PROJECT_CPD_INST_OPERATORS} ibm-cpd-datastage-operator.v8.1.0
```

Recreate datastage subscription for DataStage Enterprise Plus:

```
cat << EOF | oc apply -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: ibm-cpd-datastage-operator
  namespace: cp4d-operators
spec:
  channel: v8.1
  installPlanApproval: Automatic
  name: ibm-cpd-datastage-operator
  source: ibm-cpd-datastage-ent-plus-operator-catalog
  sourceNamespace: cp4d-operators
EOF
```

Wait for a few minutes for the CSV to be ready:

```
oc -n ${PROJECT_CPD_INST_OPERATORS} get csv ibm-cpd-datastage-operator.v8.1.0 -o jsonpath="{.status.phase}"
```

Wait for datastage and pxruntime custom resources to reconcile to ‘Completed’ state:

```
oc get datastage datastage -n ${PROJECT_CPD_INST_OPERANDS}

oc get pxruntime -n ${PROJECT_CPD_INST_OPERANDS}
```

Monitor progress of both custom resources:

```
oc get datastage datastage -o yaml | grep progress
```

```
oc get pxruntime ds-px-default -o yaml | grep dsStatus
```

### IKC 5.2.1 Connectivity Cumulative Patch (manual removal)

Run the following command to edit the WKC or CCS custom resource:

```
oc patch ccs ccs-cr -n ${PROJECT_CPD_INST_OPERANDS} --type=json --patch '[{"op":"remove","path":"/spec/image_digests/wdp_connect_connection_image"},{"op":"remove","path":"/spec/image_digests/wdp_connect_connector_image"},{"op":"remove","path":"/spec/image_digests/wdp_connect_flight_image"}]'
```

Wait for operator reconciliation to complete. Run the following command to monitor the reconciliation status:

```
oc get wkc wkc-cr -n ${PROJECT_CPD_INST_OPERANDS}
```

or

```
oc get ccs ccs-cr -n ${PROJECT_CPD_INST_OPERANDS}
```

### Remove Pipelines 5.2.1 Hot Fix 1 (manual removal)

Include steps to remove Pipelines Hot Fix 1

Set the following environment variable:

```
export PROJECT_CPD_OPERATORS=cp4d-operators
export PROJECT_CPD_INSTANCE=cp4d
```

Run the following patch command with the default image:
```
oc patch catsrc -n ${PROJECT_CPD_OPERATORS} ibm-cpd-wspipelines-operator-catalog --type='json' -p='[{"op": "replace", "path": "/spec/image", "value":"icr.io/cpopen/ibm-cpd-wspipelines-operator-catalog@sha256:eadad9dcd823ffe857f4cfd2be4a3249a2e19d5ef4c48b2704db2999a6ce037a"}]'
```

Verify, that the catalog pod is up and running:

```
oc get pods -n ${PROJECT_CPD_OPERATORS} | grep ibm-cpd-wspipelines-operator-catalog
```

Delete subscription and CSV:

```
oc delete sub -n ${PROJECT_CPD_OPERATORS} ibm-cpd-wspipelines-operator-catalog-subscription

oc delete csv -n ${PROJECT_CPD_OPERATORS} ibm-cpd-wspipelines.v11.1.0
```

Recreate subscription to initiate installation, make sure to replace <PROJECT_CPD_OPERATORS> with the cpd operators namespace, such as "cp4d-operators":

```
cat << EOF | oc apply -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: ibm-cpd-wspipelines-operator-catalog-subscription
  namespace: cp4d-operators
spec:
  channel: v11.1
  installPlanApproval: Automatic
  name: ibm-cpd-wspipelines
  source: ibm-cpd-wspipelines-operator-catalog
  sourceNamespace: cp4d-operators
EOF
```

Wait a few minutes for CSV to be ready:

```
oc -n ${PROJECT_CPD_OPERATORS} get csv ibm-cpd-wspipelines.v11.1.0 -o jsonpath="{.status.phase}"
```

Output should be:

```
Succeeded
```

Wait 15-20 minutes for reconciliation process to finish:

```
oc get wspipelines -n ${PROJECT_CPD_INSTANCE}
```

Output should be:

```
NAME             VERSION   RECONCILED   STATUS       PERCENT   AGE
wspipelines-cr   5.2.1     5.2.1        Completed    100%      35m
```

After all required patches have been reverted, proceed with the next step


## 5. Upgrade shared cluster components (ibm-cert-manager,ibm-licensing) (est. 5 minutes)

Determine which project the License Service is in

```
oc get deployment -A | grep ibm-licensing-operator
```

Upgrade the Certificate manager and License Service

The License Service is in the ${PROJECT_LICENSE_SERVICE} project

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster

```
${CPDM_OC_LOGIN}
```

Upgrade shared cluster components

```
cpd-cli manage apply-cluster-components \
--release=${VERSION} \
--license_acceptance=true \
--cert_manager_ns=${PROJECT_CERT_MANAGER} \
--licensing_ns=${PROJECT_LICENSE_SERVICE} \
--case_download=false
```


Wait for the cpd-cli to return the following message before proceeding to the next step:

[SUCCESS] ... The apply-cluster-components command ran successfully

Confirm that the Certificate manager pods in the ${PROJECT_CERT_MANAGER} project are Running or Completed:

```
oc get pods --namespace=${PROJECT_CERT_MANAGER}
```

Confirm that the License Service pods are Running or Completed

```
oc get pods --namespace=${PROJECT_LICENSE_SERVICE}
```


## 6. Prepare to upgrade an instance of IBM Software Hub (est. 3 minutes)

Validate the health of your cluster, nodes, operators, and operands before proceeding with the upgrade:

```
cpd-cli health operators \
--control_plane_ns=${PROJECT_CPD_INST_OPERANDS} \
--operator_ns=${PROJECT_CPD_INST_OPERATORS}
```

Results should read "SUCCESS..."

```
cpd-cli health operands \
--control_plane_ns=${PROJECT_CPD_INST_OPERANDS} \
--include_ns=${PROJECT_CPD_INST_OPERATORS}
```

Results should read "SUCCESS..."

```
cpd-cli health cluster
```

Results should read "SUCCESS..."

```
cpd-cli health nodes
```

Results should read "SUCCESS..."

[Next, apply entitlements](https://www.ibm.com/docs/en/software-hub/5.2.x?topic=puish-applying-your-entitlements-3) (est. 1-2 minutes):

Run the apply-entitlement command for each solution that is installed or that you plan to install in this instance
Apply entitlements for CP4D (includes WKC and Mantaflow):


```
cpd-cli manage apply-entitlement \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--entitlement=cpd-enterprise
```

Apply entitlements for DataStage:

```
cpd-cli manage apply-entitlement \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--entitlement=datastage
```


## 7. Upgrade an instance of IBM Software Hub

Upgrade the required operators and custom resources for the instance:

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:

```
${CPDM_OC_LOGIN}
```

Review the license terms for the software that you plan to install:

```
cpd-cli manage get-license \
--release=${VERSION} \
--license-type=EE
```

Upgrade the required operators and custom resources for the instance (est. 60 minutes):

```
cpd-cli manage setup-instance \
--license_acceptance=true \
--release=${VERSION} \
--cpd_operator_ns=${PROJECT_CPD_INST_OPERATORS} \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--run_storage_tests=false \
--case_download=false \
--block_storage_class=${STG_CLASS_BLOCK}
```

You can optionally run this command with the run_storage_tests flag set to 'true':

```
--run_storage_tests=true
```

Monitor the upgrade progress of the custom resources with the following commands:

```
oc get ZenService lite-cr -n cp4d -o yaml
```

```
oc get Ibmcpd ibmcpd-cr -n cp4d -o yaml
```

Upgrade the operators in the operators project (est. 32 minutes):

```
cpd-cli manage apply-olm \
--release=${VERSION} \
--cpd_operator_ns=${PROJECT_CPD_INST_OPERATORS} \
--upgrade=true \
--case_download=false
```

Wait for the cpd-cli to return the following message before proceeding to the next step:

[SUCCESS]... The apply-olm command ran successfully

Confirm that the operator pods are Running or Completed:

```
oc get pods --namespace=${PROJECT_CPD_INST_OPERATORS}
```


## 8. Upgrade CPD services (datastage_ent_plus,ws_pipelines,wkc,mantaflow)

### Upgrade WKC custom resource (est. 92 minutes):

Set the IKC_TYPE environment variable:

```
export IKC_TYPE=wkc
```

When you upgrade IBM Knowledge Catalog, the options that you specified when you installed IBM Knowledge Catalog are used

Ensure you have specified the correct installation options in the install-options.yml in the cpd-cli work directory (For example: cpd-cli-workspace/olm-utils-workspace/work)

IBM Knowledge Catalog - The following values were extracted from the latest revision of HUK internal runbook:

```
################################################################################
# IBM Knowledge Catalog parameters
################################################################################
custom_spec:
  wkc:
#    enableDataQuality: True
#    enableSemanticAutomation: False
#    enableKnowledgeGraph: True
#    useFDB: True
```

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:

```
${CPDM_OC_LOGIN}
```

Update the custom resource for IBM Knowledge Catalog:

```
cpd-cli manage apply-cr \
--components=${IKC_TYPE} \
--release=${VERSION} \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--param-file=/tmp/work/install-options.yml \
--license_acceptance=true \
--upgrade=true \
--case_download=false
```

IBM Knowledge Catalog is upgraded when the apply-cr command returns:

[SUCCESS]... The apply-cr command ran successfully

If you want to confirm that the custom resource status is Completed, you can run the cpd-cli manage get-cr-status command:

```
cpd-cli manage get-cr-status \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--components=${IKC_TYPE} \
--case_download=false
```

Monitor the status of the WKC custom resource for 'Completed':

```
oc get WKC wkc-cr -o yaml
```

Once WKC custom resource has fully reconciled, apply the 5.2.2 Connectivity patch

Save a copy of the CCS CR currently deployed on the cluster:

```
oc get ccs ccs-cr -o yaml -n ${PROJECT_CPD_INST_OPERANDS} > ccs_cr_backup_<date>.yaml 
```

To install apply the hotfix on CCS operator, run the following command to apply the patch to the CCS custom resource (ccs-cr): 

```
oc patch ccs ccs-cr -n ${PROJECT_CPD_INST_OPERANDS} --type=merge -p '{"spec":{"image_digests":{"wdp_connect_connection_image":"sha256:84f38bbdd06241313142d72f815f3ad918c82f05b69dd5e5377826e04fecbe38", "wdp_connect_connector_image":"sha256:81b070a09362bfec7e16c31a6f5c1cd70bc6be8765532a747a2fffaed9894104", "wdp_connect_flight_image":"sha256:e804203b0306e29fd91941fa318d38e6eecf8c10219994498feca3e870a99311"}}}'
```

In the command, replace ${PROJECT_CPD_INST_OPERANDS} with the project or namespace name where Watson Knowledge Catalog has been installed.

Wait for the CCS operator reconciliation to complete. Run the following command to monitor the reconciliation status: 

```
oc get ccs ccs-cr -n ${PROJECT_CPD_INST_OPERANDS}
```

After a 5-10 minutes, the hotfix image pod should be up and running with the patched image.


### Upgrade MANTA Automated Data Lineage custom resource (est. 6 minutes):

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:

```
${CPDM_OC_LOGIN}
```

Run the following commands to remove migration from the MANTA Automated Data Lineage custom resource:

```
oc edit mantaflow mantaflow-wkc
```

Remove the migration section from the custom resource, if applicable:

```
  migrations:
    h2-format-3: true
```

Then delete the following deployments from Zen namespace:

```
oc delete deploy manta-admin-gui manta-configuration-service manta-dataflow -n zen
```

Update the custom resource for MANTA Automated Data Lineage:

```
cpd-cli manage apply-cr \
--components=mantaflow \
--release=${VERSION} \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--license_acceptance=true \
--upgrade=true \
--case_download=false
```

Monitor the status of the MANTA custom resource:

```
oc get MantaFlow mantaflow-wkc -o yaml
```

MANTA Automated Data Lineage is upgraded when the apply-cr command returns:

[SUCCESS]... The apply-cr command ran successfully

If you want to confirm that the custom resource status is Completed, you can run the cpd-cli manage get-cr-status command:

```
cpd-cli manage get-cr-status \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--components=mantaflow \
--case_download=false
```


### Upgrade IBM Orchestration Pipelines custom resource (est. 14 minutes):

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:

```
${CPDM_OC_LOGIN}
```

Update the custom resource for IBM Orchestration Pipelines:

```
cpd-cli manage apply-cr \
--components=ws_pipelines \
--release=${VERSION} \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--license_acceptance=true \
--upgrade=true \
--case_download=false
```

Monitor the status of the Pipelines custom resource:

```
oc get WSPipelines wspipelines-cr -o yaml
```

IBM Orchestration Pipelines is upgraded when the apply-cr command returns:

[SUCCESS]... The apply-cr command ran successfully

If you want to confirm that the custom resource status is Completed, you can run the cpd-cli manage get-cr-status command:

```
cpd-cli manage get-cr-status \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--components=ws_pipelines
```

After Orchestration Pipelines upgrade is complete, apply the Orchestration Pipelines 5.2.2 HotFix

Export the following environment variables:

```
export PROJECT_CPD_OPERATORS=cp4d-operators
export PROJECT_CPD_INSTANCE=cp4d
```

Create a backup of the current catalog image:

```
oc -n $PROJECT_CPD_OPERATORS get CatalogSources ibm-cpd-wspipelines-operator-catalog -o=jsonpath="{.spec.image}"
```

Patch Orchestration Pipelines catalog source:

```
oc patch catsrc -n ${PROJECT_CPD_OPERATORS} ibm-cpd-wspipelines-operator-catalog --type='json' -p='[{"op": "replace", "path": "/spec/image", "value":"icr.io/cpopen/ibm-cpd-wspipelines-operator-catalog@sha256:f063d55e085568eca73015f72807290317fe09da8620b5d53d8eebe321bf7d7d"}]'
```

Verify that the catalog pod is up and running:

```
oc get pods -n ${PROJECT_CPD_OPERATORS} | grep ibm-cpd-wspipelines-operator-catalog
```

Delete subscription and CSV:

```
oc delete sub -n ${PROJECT_CPD_OPERATORS} ibm-cpd-wspipelines-operator-catalog-subscription
oc delete csv -n ${PROJECT_CPD_OPERATORS} ibm-cpd-wspipelines.v11.2.0
```

Recreate subscription to initiate installation, but make sure to replace <PROJECT_CPD_OPERATORS> with the cpd operators project:

```
cat << EOF | oc apply -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: ibm-cpd-wspipelines-operator-catalog-subscription
  namespace: cp4d-operators
spec:
  channel: v11.2
  installPlanApproval: Automatic
  name: ibm-cpd-wspipelines
  source: ibm-cpd-wspipelines-operator-catalog
  sourceNamespace: cp4d-operators
EOF
```

Wait a few minutes for CSV to be ready:

```
oc -n ${PROJECT_CPD_OPERATORS} get csv ibm-cpd-wspipelines.v11.1.0 -o jsonpath="{.status.phase}"
```

Output should be:

```
Succeeded
```

Add the hotfix label to the CSV:

```
oc label csv ibm-cpd-wspipelines.v11.2.0 -n $PROJECT_CPD_OPERATORS support.operator.ibm.com/hotfix=true
```

To verify the newly-added label, please check:

```
oc get csv ibm-cpd-wspipelines.v11.2.0 -n $PROJECT_CPD_OPERATORS -o jsonpath='{.metadata.labels}'
```

Wait 15 to 20 minutes for reconciliation process to finish:

```
oc -n ${PROJECT_CPD_INSTANCE} get wspipelines
```

Output should be:

```
NAME             VERSION   RECONCILED   STATUS       PERCENT   AGE
wspipelines-cr   5.2.2     5.2.2        Completed    100%      35m
```

### Upgrade DataStage custom resource (est. 11 minutes):

***Potential Issue During DataStage Custom Resource Upgrade***

During the previous upgrade we encountered an issue in which the DataStage operator pod was crashing, for example:

```
oc get po -n $PROJECT_CPD_INST_OPERATORS | grep datastage-operator
```

```
ibm-cpd-datastage-operator-665cd9c4d8-9mxw9                       0/1     CrashLoopBackOff     8          77h
```

Upon closer inspection of the datastage operator pod, we discovered the pod was crashing due to an OOMKilled error:

```
oc describe pod ibm-cpd-datastage-operator-665cd9c4d8-9mxw9 -n $PROJECT_CPD_INST_OPERANDS
```

Under the status -> conditions section, look for any errors, such as OOMKilled

In order to address this, increase the datastage operator pod memory by updating the memory requests/limits in the CSV:

```
oc get csv -A | grep datastage
```

```
oc edit csv ibm-cpd-datastage-operator.v8.0.0
```

Increase the memory limit from 1Gi to 2Gi:

```
resources:
  limits:
	cpu: 500m
	memory: 2Gi

```

Wait a few minutes for the changes to be applied before proceeding with the upgrade

Set the ${DATASTAGE_TYPE} variable to the edition of DataStage that you want to install:

```
export DATASTAGE_TYPE=datastage_ent_plus
```

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:

```
${CPDM_OC_LOGIN}
```

Update the custom resource for DataStage:

```
cpd-cli manage apply-cr \
--components=${DATASTAGE_TYPE} \
--release=${VERSION} \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--license_acceptance=true \
--upgrade=true \
--case_download=false
```


Monitor the status of the DataStage custom resource:

```
oc get DataStage datastage -o yaml
```

DataStage is upgraded when the apply-cr command returns:

[SUCCESS]... The apply-cr command ran successfully

Note: The service instances are automatically upgraded when you upgrade DataStage

If you want to confirm that the custom resource status is Completed, you can run the cpd-cli manage get-cr-status command:

```
cpd-cli manage get-cr-status \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--components=datastage_ent_plus
```

After DataStage upgrade is complete, apply DataStage 5.2.2 Patch 1

Run the following commands to back up datastage and pxruntime custom resources:

```
oc get datastage datastage -n ${PROJECT_CPD_INST_OPERANDS} -o yaml > datastage_backup.yaml
```

```
oc get pxruntime [instance-name] -n ${PROJECT_CPD_INST_OPERANDS} -o yaml > [instance_name]_backup.yaml
```

Run the following command to back up the current datastage catalog source image for datastage enterprise plus edition:

```
oc -n ${PROJECT_CPD_INST_OPERATORS} get CatalogSources ibm-cpd-datastage-ent-plus-operator-catalog -o=jsonpath="{.spec.image}"
```

Wait for datastage catalog source pod is up and running:  

```
oc get pod -n ${PROJECT_CPD_INST_OPERATORS} | grep ibm-cpd-datastage | grep catalog
```

Delete datastage subscription and CSV:

```
oc delete sub -n ${PROJECT_CPD_INST_OPERATORS} ibm-cpd-datastage-operator
```

```
oc delete csv -n ${PROJECT_CPD_INST_OPERATORS} ibm-cpd-datastage-operator.v8.2.0 v8.2.0
```

Recreate datastage subscription for datastage enterprise plus edition, run:

```
cat << EOF | oc apply -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: ibm-cpd-datastage-operator
  namespace: ${PROJECT_CPD_INST_OPERATORS}
spec:
  channel: v8.2
  installPlanApproval: Automatic
  name: ibm-cpd-datastage-operator
  source: ibm-cpd-datastage-ent-plus-operator-catalog
  sourceNamespace: ${PROJECT_CPD_INST_OPERATORS}
EOF
```

Wait for a few minutes for the CSV to be ready: 

```
oc -n ${PROJECT_CPD_INST_OPERATORS} get csv ibm-cpd-datastage-operator.v8.2.0 -o jsonpath="{.status.phase}"
```

Wait for datastage and pxruntime custom resources to reconcile to ‘Completed’ state:

```
oc get datastage datastage -n ${PROJECT_CPD_INST_OPERANDS}
```

```
oc get pxruntime -n ${PROJECT_CPD_INST_OPERANDS}
```

This marks the end of the datastage upgrade procedure


### Post-upgrade of all custom resources (est. <1 minute):

After all custom resources have been updated, re-enable the zen-rsi-evictor-cron-job CronJob:

```
oc patch CronJob zen-rsi-evictor-cron-job \
--namespace=${PROJECT_CPD_INST_OPERANDS} \
--type=merge \
--patch='{"spec":{"suspend": false}}'
```


## 9. Upgrade the cpdbr service:

Upgrade the cpdbr-tenant component for the instance

The command that you run depends on the type of storage that you use, where the cluster pulls images from, and whether the scheduling service is installed

For IBM Fusion environments without the scheduling service, run the following commands:

```
export CPDBR_OADP_IMAGE_VERSION=${VERSION}.amd64
export SPP_CPDBR_IMAGE_NAME=cpdbr-oadp:${CPDBR_OADP_IMAGE_VERSION}
cpd-cli oadp client config set namespace=ibm-backup-restore
cpd-cli oadp client config set cpd-namespace=cp4d
export OADP_PROJECT=ibm-backup-restore
export PROJECT_CPD_INST_OPERATORS=cp4d-operators
export PRIVATE_REGISTRY_LOCATION="nexus.lan.huk-coburg.de:2743"
```

```
cpd-cli oadp install \
--component=cpdbr-tenant \
--cpdbr-hooks-image-prefix=${PRIVATE_REGISTRY_LOCATION} \
--tenant-operator-namespace=${PROJECT_CPD_INST_OPERATORS} \
--cpdbr-hooks-image-prefix=${PRIVATE_REGISTRY_LOCATION}/cpopen/cpd \
--cpfs-image-prefix=${PRIVATE_REGISTRY_LOCATION}/cpopen/cpfs \
--upgrade=true \
--log-level=debug \
--verbose
```


## 10. Upgrade privileged monitors (est. 2 minutes)

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:

```
${CPDM_OC_LOGIN}
```

Upgrade the privileged monitors

The command that you run depends on whether you want the Operator namespace status check to include information about the scheduling service

To include only information about the operators project run the following command:

```
cpd-cli manage apply-privileged-monitoring-service \
--privileged_service_ns=${PROJECT_PRIVILEGED_MONITORING_SERVICE} \
--cpd_operator_ns=${PROJECT_CPD_INST_OPERATORS} \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS}
```


## 11. Upgrade the configuration admission controller webhook (est. 2 minutes):

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:

```
${CPDM_OC_LOGIN}
```

Upgrade the configuration admission controller webhook for x86_64 clusters:

```
cpd-cli manage install-cpd-config-ac \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--cpd_config_ac_image=${PRIVATE_REGISTRY_LOCATION}/cpopen/cpd/zen-rsi-adm-controller:${VERSION}.amd64 \
--case_download=false
```


## 12. Validate CPD upgrade (customer acceptance test):

End of document
