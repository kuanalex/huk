## HUK CPD upgrade 5.2.0 to 5.2.1

## Author: Alex Kuan (alex.kuan@ibm.com)

From:

```
CPD: 5.2.0
OCP: 4.18
Storage: Spectrum Scale 2.10.1
Internet: proxy
Private container registry: yes
Components: ibm-cert-manager,ibm-licensing,cpfs,cpd_platform,datastage_ent_plus,ws_pipelines,wkc,mantaflow
```

To:

```
CPD: 5.2.1
OCP: 4.18
Storage: Spectrum Scale 2.10.1
Internet: proxy
Private container registry: yes
Components: ibm-cert-manager,ibm-licensing,cpfs,cpd_platform,datastage_ent_plus,ws_pipelines,wkc,mantaflow
```

Upgrade flow and steps

```
1. CPD 5.2.0 pre-check
2. Update cpd-cli and environment variables script for 5.2.1
3. Backup CPD 5.2.0 CRs, cpd-instance, and cpd-operators namespaces
4. Revert any patches on the current installation
5. Upgrade shared cluster components (ibm-cert-manager,ibm-licensing)
6. Prepare to upgrade an instance of IBM Software Hub
7. Upgrade an instance of IBM Software Hub
8. Upgrade CPD services (datastage_ent_plus,ws_pipelines,wkc,mantaflow)
9. Upgrade the cpdbr service
10. Upgrade privileged monitors
11. Upgrade the configuration admission controller webhook
12. Validate CPD upgrade (customer acceptance test)
```


## 1. CPD 5.2.0 pre-check

Use a client workstation with internet (bastion or infra node) to download OCP and CPD images, and confirm the OS level, ensuring the OS is RHEL 8/9

```
cat /etc/redhat-release
```

Test internet connection, and make sure the output from the target URL and it can be connected successfully:

```
curl -v https://github.com/IBM
```

Prepare customer\'s IBM entitlement key

Log in to <https://myibm.ibm.com/products-services/containerlibrary> with the IBMid and password that are associated with the entitled software.

On the Get entitlement key tab, select Copy key to copy the entitlement key to the clipboard.

Save the API key in a text file.

Make sure free disk space more than 500 GB (to download images and pack the images into a tar ball)

```
df -lh
```

Collect OCP and CPD cluster information

Log into OCP cluster from bastion node

```
oc login $(oc whoami --show-server) -u kubeadmin -p <kubeadmin-password>
```

Review OCP version

```
oc get clusterversion
```

Review storage classes

```
oc get sc
```

Review OCP cluster status

Make sure all nodes are in ready status

```
oc get nodes
```

Make sure all mc are in correct status, UPDATED all True, UPDATING all False, DEGRADED all False

```
oc get mcp
```

Make sure all co are in correct status, AVAILABLE all True, PROGRESSING all False, DEGRADED all False

```
oc get co
```

Make sure there are no unhealthy pods, if there are, please open an IBM support case to fix them.

```
oc get po -A -owide | egrep -v '([0-9])/\1' | egrep -v 'Completed' 
```

Get CPD installed projects

```
oc get pod -A | grep zen
```

Get CPD version and installed components

```
cpd-cli manage login-to-ocp \
--username=${OCP_USERNAME} \
--password=${OCP_PASSWORD} \
--server=${OCP_URL}
```

```
cpd-cli manage get-cr-status --cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS}
```

or

```
cpd-cli manage list-deployed-components --cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS}
```

Check the scheduling service, if it is installed but not in ibm-common-services project, uninstall it

```
oc get scheduling -A
```

Check install plan is automatic

```
oc get ip -A
```

Check the spec of each CPD custom resource, remove any patches before upgrading

```
oc project ${PROJECT_CPD_INST_OPERANDS}
```

```
for i in $(oc api-resources | grep cpd.ibm.com | awk '{print $1}'); do echo "************* $i *************"; for x in $(oc get $i --no-headers | awk '{print $1}'); do echo "--------- $x ------------"; oc get $i $x -o jsonpath={.spec} | jq; done ; done
```

Probe IBM registry (if required)

```
podman login cp.icr.io -u cp -p ${IBM_ENTITLEMENT_KEY}
```


## 2. Update cpd-cli and environment variables script for 5.2.1

Download and unpack the latest cpd-cli release

```
wget https://github.com/IBM/cpd-cli/releases/download/v14.2.1/cpd-cli-linux-EE-14.2.1.tgz && gzip -d cpd-cli-linux-EE-14.2.1.tgz && tar -xvf cpd-cli-linux-EE-14.2.1.tar && rm -rf cpd-cli-linux-EE-14.2.1.tar
```

Add the cpd-cli to your PATH variable, for example

```
export PATH=/root/cpd-cli-linux-EE-14.2.1-2542:$PATH
```

Update your environment variables script

```
vi cpd_vars.sh
```

Update the Version field and save your changes

```
VERSION=5.2.1
```

Source your environment variables

```
source cpd_vars.sh
```

Launch olm-utils-play-v3 container

```
cpd-cli manage restart-container
```


## 3. Backup CPD 5.2.0 CRs, cpd-instance, and cpd-operators namespaces

Create a new directory and store the output of the following commands in that directory

```
mkdir cpdbackup && cd cpdbackup && oc project ${PROJECT_CPD_INST_OPERANDS}
for i in $(oc api-resources | grep cpd.ibm.com | awk '{print $1}'); do echo "************* $i *************"; for x in $(oc get $i --no-headers | awk '{print $1}'); do echo "--------- $x ------------"; oc get $i $x -oyaml > bak-$x.yaml; done ; done
```

**Note: The following 'oc adm' commands can be time-consuming and should be collected during the pre-upgrade Health Check activity**

Backup the current state of operands in your backup folder of choice:

```
mkdir operandsbackup && cd operandsbackup && oc adm inspect -n ${PROJECT_CPD_INST_OPERANDS} --dest-dir=source-${PROJECT_CPD_INST_OPERANDS} $(oc api-resources --namespaced=true --verbs=get,list --no-headers -o name | tr '\n' ',' | sed 's/,$//')
```

Backup the current state of operators in your backup folder of choice:

```
mkdir operatorsbackup && cd operatorsbackup && oc adm inspect -n ${PROJECT_CPD_INST_OPERATORS} --dest-dir=source-${PROJECT_CPD_INST_OPERATORS} $(oc api-resources --namespaced=true --verbs=get,list --no-headers -o name | tr '\n' ',' | sed 's/,$//')
```


## 4. Revert Patches And Hot Fixes

It is recommended to remove patches or hot fixes before upgrading Cloud Pak for Data to avoid potential conflicts, inconsistencies, or compatibility issues that may arise during the upgrade process. 

Patches and hot fixes are typically applied to address specific issues or vulnerabilities, but they might not be fully compatible with the new CP4D version. 

Removing them before upgrading ensures a cleaner and more predictable upgrade experience, reducing the risk of unexpected behavior or errors. 

The current list of patches that need to be reverted/removed are the following:
- Day0 Patch IBM SW HUB - 26.06.2025 (
- Day0 Patch Datastage Pipelines - 26.06.2025
- Pipelines; Orchestration Pipeline-HF1 - 14.07.2025
- Connectivity Patch IKC.5.2.0 (precondition for DS patch 2)
- DataStage Patch 2 - 14.07.2025
- DataStage Patch 3 - 28.07.2025
- 5.2.0 Job Run Dashboard Patch 2 = Jobs CCS 5.2.0 HotFix for HUK
- Datastage Patch 4

### Remove Orchestration Pipeline Hot fix 1 (manual removal)

In Step 1 of the patch instructions, you would have taken a backup of the previous catalog image:

```
oc -n $PROJECT_CPD_OPERATORS get CatalogSources ibm-cpd-wspipelines-operator-catalog -o=jsonpath="{.spec.image}"
```

Here is an example obtained from the 'last-applied-configuration' section of the  ibm-cpd-wspipelines-operator-catalog yaml:

```
icr.io/cpopen/ibm-cpd-wspipelines-operator-catalog@sha256:d42d4e62935e690f45e41345d9292705f75fb6383c77ab689ccc75116d4e1c42
```

In the following section, you will need to restore either a backup of the previous image or default image value

Set the PROJECT_CPD_OPERATORS environment variable:

```
export PROJECT_CPD_OPERATORS=cpd-operators
export PROJECT_CPD_INSTANCE=cpd-instance
```

Patch the pipelines catalog source using the backup of the current image or the default image value:

```
oc patch catsrc -n ${PROJECT_CPD_OPERATORS} ibm-cpd-wspipelines-operator-catalog --type='json' -p='[{"op": "replace", "path": "/spec/image", "value":"<IMAGE>"}]'
```

For example:

```
oc patch catsrc -n ${PROJECT_CPD_OPERATORS} ibm-cpd-wspipelines-operator-catalog --type='json' -p='[{"op": "replace", "path": "/spec/image", "value":"icr.io/cpopen/ibm-cpd-wspipelines-operator-catalog@sha256:d42d4e62935e690f45e41345d9292705f75fb6383c77ab689ccc75116d4e1c42"}]'
```

Verify that the catalog pod is up and running:

```
oc get pods -n ${PROJECT_CPD_OPERATORS} | grep ibm-cpd-wspipelines-operator-catalog
```

Delete the subscription:

```
oc delete sub -n ${PROJECT_CPD_OPERATORS} ibm-cpd-wspipelines-operator-catalog-subscription
```

Delete the CSV:

```
oc delete csv -n ${PROJECT_CPD_OPERATORS} ibm-cpd-wspipelines.v11.0.0
```

Recreate the subscription:

```
cat << EOF | oc apply -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: ibm-cpd-wspipelines-operator-catalog-subscription
  namespace: $PROJECT_CPD_OPERATORS
spec:
  channel: v11.0
  installPlanApproval: Automatic
  name: ibm-cpd-wspipelines
  source: ibm-cpd-wspipelines-operator-catalog
  sourceNamespace: $PROJECT_CPD_OPERATORS
EOF
```

Wait for the CSV to be ready -> Output should be 'Succeeded':

```
oc get csv ibm-cpd-wspipelines.v11.0.0 -o jsonpath="{.status.phase}" -n ${PROJECT_CPD_OPERATORS}
```

Wait 15-20 minutes for the reconciliation process to finish -> wspipelines-cr Status should be 'Completed' and Percent should be '100%':

```
oc get wspipelines -n ${PROJECT_CPD_INSTANCE}
```

### Remove DataStage Patch 4 (manual removal)

Remove the image digests updates from the DataStage custom resource:

```
oc patch datastage datastage -n ${PROJECT_CPD_INST_OPERANDS} --type=json --patch '[{"op": "remove", "path":"/spec/image_digests"}]'
```

Wait for the DataStage operator reconciliation to complete:

```
oc get datastage datastage -n ${PROJECT_CPD_INST_OPERANDS}
```

After some time, the datastage pods in ${PROJECT_CPD_INST_OPERANDS} should be up and running with the original image

Get the pxruntime instance name:

```
oc get pxruntime
```

Run the following command to remove image digest updates from the DataStage custom resource:

```
oc patch pxruntime [pxruntime instance name] -n ${PROJECT_CPD_INST_OPERANDS} --type=json --patch '[{"op": "remove", "path":"/spec/image_digests"}]'
```

Wait for the DataStage operator reconciliation to complete:

```
oc get pxruntime [instance-name] -n ${PROJECT_CPD_INST_OPERANDS}
```

After some time, the datastage pods in ${PROJECT_CPD_INST_OPERANDS} should be up and running with the original image


### Remove IKC 5.2.0 Connectivity Patch (manual removal)

Run the following command to edit the WKC or CCS custom resource:

```
oc patch ccs ccs-cr -n ${PROJECT_CPD_INST_OPERANDS} --type=json --patch'[{"op":"remove","path":"/spec/image_digests/wdp_connect_connection_image"},{"op":"remove","path":"/spec/image_digests/wdp_connect_connector_image"},{"op":"remove","path":"/spec/image_digests/wdp_connect_flight_image"}]'
```

Wait for operator reconciliation to complete

Run the following command to monitor the reconciliation status:

```
oc get wkc wkc-cr -n ${PROJECT_CPD_INST_OPERANDS}
```

or

```
oc get ccs ccs-cr -n ${PROJECT_CPD_INST_OPERANDS}
```

### Remove CCS 5.2.0 Hot fix (manual removal)

If jobs_api_image and/or jobs_ui_image exists in the backed-up ccs-cr from step 2a, find the old values from ccs_cr_backup_<date>.yaml and use the following command to revert to the previous versions:

```
oc patch ccs ccs-cr -n ${PROJECT_CPD_INST_OPERANDS} --type=merge -p '{"spec":{"image_digests":{"jobs_api_image":"sha256:previous_jobs_api_digest","jobs_ui_image":"sha256:previous_jobs_ui_digest"}}}'
```

Otherwise, remove the values from ccs-cr with:

```
oc patch ccs ccs-cr -n ${PROJECT_CPD_INST_OPERANDS} --type=json -p '[{"op": "remove", "path": "/spec/image_digests/jobs_api_image"},{"op": "remove", "path": "/spec/image_digests/jobs_ui_image"}]'
```

Wait for operator reconciliation to complete

Run the following command to monitor the reconciliation status:

```
oc get ccs ccs-cr -n ${PROJECT_CPD_INST_OPERANDS}
```

### Remove Day0 Patch for IBM Software Hub (manual removal)

Log in to the Red Hat OpenShift Container Platform cluster where IBM Software Hub Version 5.2.0 is installed

Download the attached script, 5.2.0-day0-patch-v5.sh, to your client workstation:

```
https://www.ibm.com/support/pages/node/7236425
```

Make the script executable:

```
chmod 755 5.2.0-day0-patch-v5.sh
```

To remove the patch, run the following command:

```
nohup ./5.2.0-day0-patch-v5.sh \
--operator ${PROJECT_CPD_INST_OPERATORS} \
--remove \
--yes > patch_remove_output.log &
```

In a 2nd terminal window, monitor the patch removal progress in the patch_remove_output.log file:

```
tail -f patch_remove_output.log
```

Look for confirmation that the patch removal was successful:

```
Remove Day 0 Patch has been successfully completed
```

After all required patches have been reverted, proceed with the next step


## 5. Upgrade shared cluster components (ibm-cert-manager,ibm-licensing) (est. 5 minutes)

Determine which project the License Service is in

```
oc get deployment -A | grep ibm-licensing-operator
```

Upgrade the Certificate manager and License Service

The License Service is in the ${PROJECT_LICENSE_SERVICE} project

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster

```
${CPDM_OC_LOGIN}
```

Upgrade shared cluster components

```
cpd-cli manage apply-cluster-components \
--release=${VERSION} \
--license_acceptance=true \
--cert_manager_ns=${PROJECT_CERT_MANAGER} \
--licensing_ns=${PROJECT_LICENSE_SERVICE}
```


Wait for the cpd-cli to return the following message before proceeding to the next step:

[SUCCESS] ... The apply-cluster-components command ran successfully

Confirm that the Certificate manager pods in the ${PROJECT_CERT_MANAGER} project are Running or Completed:

```
oc get pods --namespace=${PROJECT_CERT_MANAGER}
```

Confirm that the License Service pods are Running or Completed

```
oc get pods --namespace=${PROJECT_LICENSE_SERVICE}
```


## 6. Prepare to upgrade an instance of IBM Software Hub (est. 3 minutes)

Validate the health of your cluster, nodes, operators, and operands before proceeding with the upgrade:

```
cpd-cli health operators \
--control_plane_ns=${PROJECT_CPD_INST_OPERANDS} \
--operator_ns=${PROJECT_CPD_INST_OPERATORS}
```

Results should read "SUCCESS..."

```
cpd-cli health operands \
--control_plane_ns=${PROJECT_CPD_INST_OPERANDS} \
--include_ns=${PROJECT_CPD_INST_OPERATORS}
```

Results should read "SUCCESS..."

```
cpd-cli health cluster
```

Results should read "SUCCESS..."

```
cpd-cli health nodes
```

Results should read "SUCCESS..."

[Next, apply entitlements](https://www.ibm.com/docs/en/software-hub/5.2.x?topic=puish-applying-your-entitlements-3) (est. 1-2 minutes):

Run the apply-entitlement command for each solution that is installed or that you plan to install in this instance

Apply entitlements for CP4D:

```
cpd-cli manage apply-entitlement \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--entitlement=cpd-enterprise
```

Apply entitlements for DataStage:

```
cpd-cli manage apply-entitlement \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--entitlement=datastage
```

Apply entitlements for IKC/WKC:

```
cpd-cli manage apply-entitlement \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--entitlement=ikc-standard
```


## 7. Upgrade an instance of IBM Software Hub

Upgrade the required operators and custom resources for the instance:

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:

```
${CPDM_OC_LOGIN}
```

Review the license terms for the software that you plan to install:

```
cpd-cli manage get-license \
--release=${VERSION} \
--license-type=EE
```

Upgrade the required operators and custom resources for the instance (est. 60 minutes):

```
cpd-cli manage setup-instance \
--license_acceptance=true \
--release=${VERSION} \
--cpd_operator_ns=${PROJECT_CPD_INST_OPERATORS} \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--run_storage_tests=false
```

You can optionally run this command with the run_storage_tests flag set to 'true':

```
--run_storage_tests=true
```

Monitor the upgrade progress of the custom resources with the following commands:

```
oc get ZenService lite-cr -n cpd-instance -o yaml
```

```
oc get Ibmcpd ibmcpd-cr -n cpd-instance -o yaml
```

Upgrade the operators in the operators project (est. 32 minutes):

```
cpd-cli manage apply-olm \
--release=${VERSION} \
--cpd_operator_ns=${PROJECT_CPD_INST_OPERATORS} \
--upgrade=true
```

Wait for the cpd-cli to return the following message before proceeding to the next step:

[SUCCESS]... The apply-olm command ran successfully

Confirm that the operator pods are Running or Completed:

```
oc get pods --namespace=${PROJECT_CPD_INST_OPERATORS}
```


## 8. Upgrade CPD services (datastage_ent_plus,ws_pipelines,wkc,mantaflow)

### Upgrade WKC custom resource (est. 92 minutes):

Set the IKC_TYPE environment variable:

```
export IKC_TYPE=wkc
```

When you upgrade IBM Knowledge Catalog, the options that you specified when you installed IBM Knowledge Catalog are used

Ensure you have specified the correct installation options in the install-options.yml in the cpd-cli work directory (For example: cpd-cli-workspace/olm-utils-workspace/work)

IBM Knowledge Catalog - The following values were extracted from the latest revision of HUK internal runbook:

```
################################################################################
# IBM Knowledge Catalog parameters
################################################################################
custom_spec:
  wkc:
#    enableDataQuality: True
#    enableSemanticAutomation: False
#    enableKnowledgeGraph: True
#    useFDB: True
```

If you applied any patches to your current installation of IBM Knowledge Catalog, check the patch instructions for cleanup steps and complete these before you start the upgrade

The patch instructions will contain cleanup instructions similar to the ones in this example: 

[Installing the patch for version 5.0.3 in the IBM Cloud Pak® for Data 5.0 documentation.](https://www.ibm.com/docs/en/cloud-paks/cp-data/5.0.x?topic=setup-installing-patch-version-503)

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:

```
${CPDM_OC_LOGIN}
```

Update the custom resource for IBM Knowledge Catalog:

```
cpd-cli manage apply-cr \
--components=${IKC_TYPE} \
--release=${VERSION} \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--license_acceptance=true \
--upgrade=true
```


IBM Knowledge Catalog is upgraded when the apply-cr command returns:

[SUCCESS]... The apply-cr command ran successfully

If you want to confirm that the custom resource status is Completed, you can run the cpd-cli manage get-cr-status command:

```
cpd-cli manage get-cr-status \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--components=${IKC_TYPE}
```

Monitor the status of the WKC custom resource:

```
oc get WKC wkc-cr -o yaml
```

When the WKC custom resource upgrade progress reaches 50%, you may observe the following messages:

```
message:
      One or more items failed
      Failed to deploy WKC
      Failed at task: Wait until the service is ready - Item: wdp-policy-service
      The error was: One or more items failed
	  
"Failed to deploy Policy service\nFailed at task:
    Wait until the service is ready - Item: wdp-policy-service\nThe error was: One
    or more items failed"'
```




### Upgrade MANTA Automated Data Lineage custom resource (est. 6 minutes):

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:

```
${CPDM_OC_LOGIN}
```

Run the following commands to remove migration from the MANTA Automated Data Lineage custom resource:

```
oc edit mantaflow mantaflow-wkc
```

Remove the migration section from the cr:

```
  migrations:
    h2-format-3: true
```

Then delete the following deployments from Zen namespace:

```
oc delete deploy manta-admin-gui manta-configuration-service manta-dataflow -n zen
```

Update the custom resource for MANTA Automated Data Lineage:

```
cpd-cli manage apply-cr \
--components=mantaflow \
--release=${VERSION} \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--license_acceptance=true \
--upgrade=true
```

Monitor the status of the MANTA custom resource:

```
oc get MantaFlow mantaflow-wkc -o yaml
```

MANTA Automated Data Lineage is upgraded when the apply-cr command returns:

[SUCCESS]... The apply-cr command ran successfully

If you want to confirm that the custom resource status is Completed, you can run the cpd-cli manage get-cr-status command:

```
cpd-cli manage get-cr-status \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--components=mantaflow
```


### Upgrade IBM Orchestration Pipelines custom resource (est. 14 minutes):

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:

```
${CPDM_OC_LOGIN}
```

Update the custom resource for IBM Orchestration Pipelines:

```
cpd-cli manage apply-cr \
--components=ws_pipelines \
--release=${VERSION} \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--license_acceptance=true \
--upgrade=true
```

Monitor the status of the Pipelines custom resource:

```
oc get WSPipelines wspipelines-cr -o yaml
```

IBM Orchestration Pipelines is upgraded when the apply-cr command returns:

[SUCCESS]... The apply-cr command ran successfully

If you want to confirm that the custom resource status is Completed, you can run the cpd-cli manage get-cr-status command:

```
cpd-cli manage get-cr-status \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--components=ws_pipelines
```

After Orchestration Pipelines upgrade is complete, apply the Orchestration Pipelines 5.2.1 HotFix

Create a backup of the current catalog image:

```
export PROJECT_CPD_OPERATORS=cpd-operators
```

```
oc -n $PROJECT_CPD_OPERATORS get CatalogSources ibm-cpd-wspipelines-operator-catalog -o=jsonpath="{.spec.image}"
```

Export the following environment variables:

```
export PROJECT_CPD_INSTANCE=cpd-instance
```

```
export PROJECT_CPD_OPERATORS=cpd-operators
```

Patch Orchestration Pipelines catalog source:

```
oc patch catsrc -n ${PROJECT_CPD_OPERATORS} ibm-cpd-wspipelines-operator-catalog --type='json' -p='[{"op": "replace", "path": "/spec/image", "value":"icr.io/cpopen/ibm-cpd-wspipelines-operator-catalog@sha256:295eeca47512295a840bc174b7448ebcc5d2e79dee87805478217d12e3a99f94"}]'
```

Verify that the catalog pod is up and running:

```
oc get pods -n ${PROJECT_CPD_OPERATORS} | grep ibm-cpd-wspipelines-operator-catalog
```

Delete subscription and CSV:

```
oc delete sub -n ${PROJECT_CPD_OPERATORS} ibm-cpd-wspipelines-operator-catalog-subscription
```

```
oc delete csv -n ${PROJECT_CPD_OPERATORS} ibm-cpd-wspipelines.v11.1.0
```

Recreate subscription to initiate installation, but make sure to replace <PROJECT_CPD_OPERATORS> with the cpd operators project:

```
cat << EOF | oc apply -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: ibm-cpd-wspipelines-operator-catalog-subscription
  namespace: <PROJECT_CPD_OPERATORS>
spec:
  channel: v11.1
  installPlanApproval: Automatic
  name: ibm-cpd-wspipelines
  source: ibm-cpd-wspipelines-operator-catalog
  sourceNamespace: <PROJECT_CPD_OPERATORS>
EOF
```

Wait a few minutes for CSV to be ready:

```
oc -n ${PROJECT_CPD_OPERATORS} get csv ibm-cpd-wspipelines.v11.1.0 -o jsonpath="{.status.phase}"
```

Output should be:

```
Succeeded
```

Add the hotfix label to the CSV:

```
oc label csv ibm-cpd-wspipelines.v11.1.0 -n $PROJECT_CPD_OPERATORS support.operator.ibm.com/hotfix=true
```

To verify the newly-added label, please check:

```
oc get csv ibm-cpd-wspipelines.v11.1.0 -n $PROJECT_CPD_OPERATORS -o jsonpath='{.metadata.labels}'
```

Restart wsp-ts pods using command:

```
oc -n ${PROJECT_CPD_INSTANCE} rollout restart deploy wsp-ts
```

Wait 15 to 20 minutes for reconciliation process to finish:

```
oc -n ${PROJECT_CPD_INSTANCE} get wspipelines
```

Output should be:

```
NAME             VERSION   RECONCILED   STATUS       PERCENT   AGE
wspipelines-cr   5.2.1     5.2.1        Completed    100%      35m
```

### Upgrade DataStage custom resource (est. 11 minutes):

Set the ${DATASTAGE_TYPE} variable to the edition of DataStage that you want to install:

```
export DATASTAGE_TYPE=datastage_ent_plus
```

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:

```
${CPDM_OC_LOGIN}
```

Update the custom resource for DataStage:

```
cpd-cli manage apply-cr \
--components=${DATASTAGE_TYPE} \
--release=${VERSION} \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--license_acceptance=true \
--upgrade=true
```

Monitor the status of the DataStage custom resource:

```
oc get DataStage datastage -o yaml
```

DataStage is upgraded when the apply-cr command returns:

[SUCCESS]... The apply-cr command ran successfully

Note: The service instances are automatically upgraded when you upgrade DataStage

If you want to confirm that the custom resource status is Completed, you can run the cpd-cli manage get-cr-status command:

```
cpd-cli manage get-cr-status \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--components=datastage_ent_plus
```

After DataStage upgrade is complete, apply DataStage 5.2.1 Patch 2

Run the following command for the datastage instance to apply the patch:

```
oc patch datastage datastage -n "${PROJECT_CPD_INST_OPERANDS}" --type merge -p '{"spec": {"image_digests": {"canvas": "sha256:02c83d398472796cda042ebd11058f32c4677263fd1150b165673c23ec74de3c","caslite": "sha256:ad2be55d3704bacc04c4c2bda1ebe3e39d1f53dd1351612c25802051346a9e3a","flows": "sha256:a4c5cd3316d6f7d3ce348801ae4d9bcaa8934c1993a601f14a2baa5e02cf1ea7","nginx": "sha256:fc655812543610c6d1ae76554e8b9927aff51110566d9d3189bafdb96ebfb30e","migration": "sha256:ccce0b203e73f8f1c5b79f9cd20e6b27098c75e4f64f4719defa4893f010fa82","assets": "sha256:ee14e70938e957ececcf0bc96a61f987502f7db9f378be87bef91ef5e9ca5d55","ruleset": "sha256:3ca33516b56a8d4a8e0fa3a8acf37f0c4531060504b42b7c4f8159b00c17612c","runtime": "sha256:9cfa40397bfb255efa3a5e5b3bfbc51ce918a27e7818c7817fe096709849b4d2","dataservice": "sha256:6134f3c8c0cc654e4c8109d73c49818bd085184c7558c2429dee241e29b64ee4","metrics": "sha256:5e61fe899c4e6f4ff3d16d60312a6e91147d5e5ba3df9a589b5692fe81549c69"}}}'
```

Wait for the DataStage operator reconciliation to complete

Run the following command to monitor the reconciliation status:

```
oc get datastage datastage -n ${PROJECT_CPD_INST_OPERANDS}
```

After some time, the respective datastage pods in ${PROJECT_CPD_INST_OPERANDS} namespace should be up and running with the updated image

Note that some images will not update based on if the previous patches were applied

Find the pxruntime instance names:

```
oc get pxruntime -n ${PROJECT_CPD_INST_OPERANDS}
```

Run the following command for each instance found in the above command to apply the patch:

```
oc patch pxruntime -n ${PROJECT_CPD_INST_OPERANDS} [instance-name] --type merge -p '{"spec":{"image_digests":{"pxcompute":"sha256:ee7c858ccca5877cb55e1b0f004e014b92713c669a2608780d64e25704eb8ba2","pxruntime":"sha256:c34704017a6af51cf5601f1cba5cf579602bf3bdcb3423fdd16bb1899069b6e3"}}}'
```

Wait for the DataStage operator reconciliation to complete. Run the following command to monitor the reconciliation status:

```
oc get pxruntime [instance-name] -n ${PROJECT_CPD_INST_OPERANDS}
```

After some time, the datastage px-runtime and px-compute pods in ${PROJECT_CPD_INST_OPERANDS} should be up and running with the updated image

If you are upgrading from pre-5.2.1 to 5.2.1 release, perform these additional steps to regenerate configmaps after the upgrade is completed

Delete the existing configmaps:

```
oc delete cm datastage-maint-aux-ckpt-cm -n ${PROJECT_CPD_INST_OPERANDS}
```

```
oc delete cm datastage-maint-aux-br-cm -n ${PROJECT_CPD_INST_OPERANDS}
```

Wait a few minutes for the configmaps to be regenerated and then run the following command to monitor the newly generated configmaps:

```
oc get cm -n cpd-instance | egrep "datastage-maint-aux-ckpt|datastage-maint-aux-br"
```

This marks the end of the datastage upgrade procedure


### Post-upgrade of all custom resources (est. <1 minute):

After all custom resources have been updated, enable the zen-rsi-evictor-cron-job CronJob:

```
oc patch CronJob zen-rsi-evictor-cron-job \
--namespace=${PROJECT_CPD_INST_OPERANDS} \
--type=merge \
--patch='{"spec":{"suspend": false}}'
```


## 9. Upgrade the cpdbr service:

Upgrade the cpdbr-tenant component for the instance

The command that you run depends on the type of storage that you use, where the cluster pulls images from, and whether the scheduling service is installed

For IBM Fusion environments without the scheduling service, run the following command:

```
cpd-cli oadp install \
--component=cpdbr-tenant \
--cpdbr-hooks-image-prefix=${PRIVATE_REGISTRY_LOCATION} \
--tenant-operator-namespace=${PROJECT_CPD_INST_OPERATORS} \
--upgrade=true \
--log-level=debug \
--verbose
```


## 10. Upgrade privileged monitors:

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:

```
${CPDM_OC_LOGIN}
```

Upgrade the privileged monitors

The command that you run depends on whether you want the Operator namespace status check to include information about the scheduling service

To include only information about the operators project run the following command:

```
cpd-cli manage apply-privileged-monitoring-service \
--privileged_service_ns=${PROJECT_PRIVILEGED_MONITORING_SERVICE} \
--cpd_operator_ns=${PROJECT_CPD_INST_OPERATORS} \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS}
```


## 11. Upgrade the configuration admission controller webhook (if required):

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:

```
${CPDM_OC_LOGIN}
```

Upgrade the configuration admission controller webhook for x86_64 clusters:

```
cpd-cli manage install-cpd-config-ac \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--cpd_config_ac_image=${PRIVATE_REGISTRY_LOCATION}/cpopen/cpd/zen-rsi-adm-controller:${VERSION}.amd64
```


## 12. Validate CPD upgrade (customer acceptance test):

End of document
