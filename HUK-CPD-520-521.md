## HUK CPD upgrade 5.2.0 to 5.2.1

## Author: Alex Kuan (alex.kuan@ibm.com)

From:

```
CPD: 5.2.0
OCP: 4.18
Storage: Spectrum Scale 2.10.1
Internet: proxy
Private container registry: yes
Components: ibm-cert-manager,ibm-licensing,cpfs,cpd_platform,datastage_ent_plus,ws_pipelines,wkc,mantaflow
```

To:

```
CPD: 5.2.1
OCP: 4.18
Storage: Spectrum Scale 2.10.1
Internet: proxy
Private container registry: yes
Components: ibm-cert-manager,ibm-licensing,cpfs,cpd_platform,datastage_ent_plus,ws_pipelines,wkc,mantaflow
```

Upgrade flow and steps

```
1. CPD 5.2.0 pre-check
2. Update cpd-cli and environment variables script for 5.2.1
3. Backup CPD 5.2.0 CRs, cpd-instance, and cpd-operators namespaces
4. Revert any patches on the current installation
5. Upgrade shared cluster components (ibm-cert-manager,ibm-licensing)
6. Prepare to upgrade an instance of IBM Software Hub
7. Upgrade an instance of IBM Software Hub
8. Upgrade CPD services (datastage_ent_plus,ws_pipelines,wkc,mantaflow)
9. Upgrade the cpdbr service
10. Upgrade privileged monitors
11. Upgrade the configuration admission controller webhook
12. Validate CPD upgrade (customer acceptance test)
```


## 1. CPD 5.2.0 pre-check

Use a client workstation with internet (bastion or infra node) to download OCP and CPD images, and confirm the OS level, ensuring the OS is RHEL 8/9

```
cat /etc/redhat-release
```

Test internet connection, and make sure the output from the target URL and it can be connected successfully:

```
curl -v https://github.com/IBM
```

Prepare customer\'s IBM entitlement key

Log in to <https://myibm.ibm.com/products-services/containerlibrary> with the IBMid and password that are associated with the entitled software.

On the Get entitlement key tab, select Copy key to copy the entitlement key to the clipboard.

Save the API key in a text file.

Make sure free disk space more than 500 GB (to download images and pack the images into a tar ball)

```
df -lh
```

Collect OCP and CPD cluster information

Log into OCP cluster from bastion node

```
oc login $(oc whoami --show-server) -u kubeadmin -p <kubeadmin-password>
```

Review OCP version

```
oc get clusterversion
```

Review storage classes

```
oc get sc
```

Review OCP cluster status

Make sure all nodes are in ready status

```
oc get nodes
```

Make sure all mc are in correct status, UPDATED all True, UPDATING all False, DEGRADED all False

```
oc get mcp
```

Make sure all co are in correct status, AVAILABLE all True, PROGRESSING all False, DEGRADED all False

```
oc get co
```

Make sure there are no unhealthy pods, if there are, please open an IBM support case to fix them.

```
oc get po -A -owide | egrep -v '([0-9])/\1' | egrep -v 'Completed' 
```

Get CPD installed projects

```
oc get pod -A | grep zen
```

Get CPD version and installed components

```
cpd-cli manage login-to-ocp \
--username=${OCP_USERNAME} \
--password=${OCP_PASSWORD} \
--server=${OCP_URL}
```

```
cpd-cli manage get-cr-status --cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS}
```

or

```
cpd-cli manage list-deployed-components --cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS}
```

Check the scheduling service, if it is installed but not in ibm-common-services project, uninstall it

```
oc get scheduling -A
```

Check install plan is automatic

```
oc get ip -A
```

Check the spec of each CPD custom resource, remove any patches before upgrading

```
oc project ${PROJECT_CPD_INST_OPERANDS}
```

```
for i in $(oc api-resources | grep cpd.ibm.com | awk '{print $1}'); do echo "************* $i *************"; for x in $(oc get $i --no-headers | awk '{print $1}'); do echo "--------- $x ------------"; oc get $i $x -o jsonpath={.spec} | jq; done ; done
```

Probe IBM registry (if required)

```
podman login cp.icr.io -u cp -p ${IBM_ENTITLEMENT_KEY}
```


## 2. Update cpd-cli and environment variables script for 5.2.1

Download and unpack the latest cpd-cli release

```
wget https://github.com/IBM/cpd-cli/releases/download/v14.2.1/cpd-cli-linux-EE-14.2.1.tgz && gzip -d cpd-cli-linux-EE-14.2.1.tgz && tar -xvf cpd-cli-linux-EE-14.2.1.tar && rm -rf cpd-cli-linux-EE-14.2.1.tar
```

Add the cpd-cli to your PATH variable, for example

```
export PATH=/root/cpd-cli-linux-EE-14.2.1-2542:$PATH
```

Update your environment variables script

```
vi cpd_vars.sh
```

Update the Version field and save your changes

```
VERSION=5.2.1
```

Source your environment variables

```
source cpd_vars.sh
```

Launch olm-utils-play-v3 container

```
cpd-cli manage restart-container
```


## 3. Backup CPD 5.2.0 CRs, cpd-instance, and cpd-operators namespaces

Create a new directory and store the output of the following commands in that directory

```
mkdir cpdbackup && cd cpdbackup && oc project ${PROJECT_CPD_INST_OPERANDS}
for i in $(oc api-resources | grep cpd.ibm.com | awk '{print $1}'); do echo "************* $i *************"; for x in $(oc get $i --no-headers | awk '{print $1}'); do echo "--------- $x ------------"; oc get $i $x -oyaml > bak-$x.yaml; done ; done
```

**Note: The following 'oc adm' commands can be time-consuming and should be collected during the pre-upgrade Health Check activity**

Backup the current state of operands in your backup folder of choice:

```
mkdir operandsbackup && cd operandsbackup && oc adm inspect -n ${PROJECT_CPD_INST_OPERANDS} --dest-dir=source-${PROJECT_CPD_INST_OPERANDS} $(oc api-resources --namespaced=true --verbs=get,list --no-headers -o name | tr '\n' ',' | sed 's/,$//')
```

Backup the current state of operators in your backup folder of choice:

```
mkdir operatorsbackup && cd operatorsbackup && oc adm inspect -n ${PROJECT_CPD_INST_OPERATORS} --dest-dir=source-${PROJECT_CPD_INST_OPERATORS} $(oc api-resources --namespaced=true --verbs=get,list --no-headers -o name | tr '\n' ',' | sed 's/,$//')
```


## 4. Revert Patches And Hot Fixes

It is recommended to remove patches or hot fixes before upgrading Cloud Pak for Data to avoid potential conflicts, inconsistencies, or compatibility issues that may arise during the upgrade process. 

Patches and hot fixes are typically applied to address specific issues or vulnerabilities, but they might not be fully compatible with the new CP4D version. 

Removing them before upgrading ensures a cleaner and more predictable upgrade experience, reducing the risk of unexpected behavior or errors. 

The current list of patches that need to be reverted/removed are the following:
- Day0 Patch IBM SW HUB - 26.06.2025 (
- Day0 Patch Datastage Pipelines - 26.06.2025
- Pipelines; Orchestration Pipeline-HF1 - 14.07.2025
- Connectivity Patch IKC.5.2.0 (precondition for DS patch 2)
- DataStage Patch 2 - 14.07.2025
- DataStage Patch 3 - 28.07.2025
- 5.2.0 Job Run Dashboard Patch 2 = Jobs CCS 5.2.0 HotFix for HUK
- Datastage Patch 4

### Remove Orchestration Pipeline Hot fix 1 (manual removal)

In Step 1 of the patch instructions, you would have taken a backup of the previous catalog image:

```
oc -n $PROJECT_CPD_OPERATORS get CatalogSources ibm-cpd-wspipelines-operator-catalog -o=jsonpath="{.spec.image}"
```

Here is an example obtained from the 'last-applied-configuration' section of the  ibm-cpd-wspipelines-operator-catalog yaml:

```
icr.io/cpopen/ibm-cpd-wspipelines-operator-catalog@sha256:d42d4e62935e690f45e41345d9292705f75fb6383c77ab689ccc75116d4e1c42
```

In the following section, you will need to restore either a backup of the previous image or default image value

Set the PROJECT_CPD_OPERATORS environment variable:

```
export PROJECT_CPD_OPERATORS=cpd-operators
export PROJECT_CPD_INSTANCE=cpd-instance
```

Patch the pipelines catalog source using the backup of the current image or the default image value:

```
oc patch catsrc -n ${PROJECT_CPD_OPERATORS} ibm-cpd-wspipelines-operator-catalog --type='json' -p='[{"op": "replace", "path": "/spec/image", "value":"<IMAGE>"}]'
```

For example:

```
oc patch catsrc -n ${PROJECT_CPD_OPERATORS} ibm-cpd-wspipelines-operator-catalog --type='json' -p='[{"op": "replace", "path": "/spec/image", "value":"icr.io/cpopen/ibm-cpd-wspipelines-operator-catalog@sha256:d42d4e62935e690f45e41345d9292705f75fb6383c77ab689ccc75116d4e1c42"}]'
```

Verify that the catalog pod is up and running:

```
oc get pods -n ${PROJECT_CPD_OPERATORS} | grep ibm-cpd-wspipelines-operator-catalog
```

Delete the subscription:

```
oc delete sub -n ${PROJECT_CPD_OPERATORS} ibm-cpd-wspipelines-operator-catalog-subscription
```

Delete the CSV:

```
oc delete csv -n ${PROJECT_CPD_OPERATORS} ibm-cpd-wspipelines.v11.0.0
```

Recreate the subscription:

```
cat << EOF | oc apply -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: ibm-cpd-wspipelines-operator-catalog-subscription
  namespace: $PROJECT_CPD_OPERATORS
spec:
  channel: v11.0
  installPlanApproval: Automatic
  name: ibm-cpd-wspipelines
  source: ibm-cpd-wspipelines-operator-catalog
  sourceNamespace: $PROJECT_CPD_OPERATORS
EOF
```

Wait for the CSV to be ready -> Output should be 'Succeeded':

```
oc get csv ibm-cpd-wspipelines.v11.0.0 -o jsonpath="{.status.phase}" -n ${PROJECT_CPD_OPERATORS}
```

Wait 15-20 minutes for the reconciliation process to finish -> wspipelines-cr Status should be 'Completed' and Percent should be '100%':

```
oc get wspipelines -n ${PROJECT_CPD_INSTANCE}
```

### Remove DataStage Patch 4 (manual removal)

Remove the image digests updates from the DataStage custom resource:

```
oc patch datastage datastage -n ${PROJECT_CPD_INST_OPERANDS} --type=json --patch '[{"op": "remove", "path":"/spec/image_digests"}]'
```

Wait for the DataStage operator reconciliation to complete:

```
oc get datastage datastage -n ${PROJECT_CPD_INST_OPERANDS}
```

After some time, the datastage pods in ${PROJECT_CPD_INST_OPERANDS} should be up and running with the original image

Get the pxruntime instance name:

```
oc get pxruntime
```

Run the following command to remove image digest updates from the DataStage custom resource:

```
oc patch pxruntime [pxruntime instance name] -n ${PROJECT_CPD_INST_OPERANDS} --type=json --patch '[{"op": "remove", "path":"/spec/image_digests"}]'
```

Wait for the DataStage operator reconciliation to complete:

```
oc get pxruntime [instance-name] -n ${PROJECT_CPD_INST_OPERANDS}
```

After some time, the datastage pods in ${PROJECT_CPD_INST_OPERANDS} should be up and running with the original image


### Remove IKC 5.2.0 Connectivity Patch and CCS 5.2.0 Hot fix (manual removal)

Run the following command to edit the CCS custom resource:

```
oc patch ccs ccs-cr -n ${PROJECT_CPD_INST_OPERANDS} --type=json --patch'[{"op":"remove","path":"/spec/image_digests/wdp_connect_connection_image"},{"op":"remove","path":"/spec/image_digests/wdp_connect_connector_image"},{"op":"remove","path":"/spec/image_digests/wdp_connect_flight_image"}, {"op": "remove", "path":"/spec/image_digests/jobs_api_image"},{"op": "remove", "path":"/spec/image_digests/jobs_ui_image"}]'
```

Wait for operator reconciliation to complete

Run the following command to monitor the reconciliation status:

```
oc get ccs ccs-cr -n ${PROJECT_CPD_INST_OPERANDS}
```

### Remove Day0 Patch for IBM Software Hub (manual removal)

Log in to the Red Hat OpenShift Container Platform cluster where IBM Software Hub Version 5.2.0 is installed

Download the attached script, 5.2.0-day0-patch-v5.sh, to your client workstation:

```
https://www.ibm.com/support/pages/node/7236425
```

Make the script executable:

```
chmod 755 5.2.0-day0-patch-v5.sh
```

To remove the patch, run the following command:

```
nohup ./5.2.0-day0-patch-v5.sh \
--operator ${PROJECT_CPD_INST_OPERATORS} \
--remove \
--yes > patch_remove_output.log &
```

In a 2nd terminal window, monitor the patch removal progress in the patch_remove_output.log file:

```
tail -f patch_remove_output.log
```

Look for confirmation that the patch removal was successful:

```
Remove Day 0 Patch has been successfully completed
```

After all required patches have been reverted, proceed with the next step


## 5. Upgrade shared cluster components (ibm-cert-manager,ibm-licensing) (est. 5 minutes)

Determine which project the License Service is in

```
oc get deployment -A | grep ibm-licensing-operator
```

Upgrade the Certificate manager and License Service

The License Service is in the ${PROJECT_LICENSE_SERVICE} project

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster

```
${CPDM_OC_LOGIN}
```

Upgrade shared cluster components

```
cpd-cli manage apply-cluster-components \
--release=${VERSION} \
--license_acceptance=true \
--cert_manager_ns=${PROJECT_CERT_MANAGER} \
--licensing_ns=${PROJECT_LICENSE_SERVICE} \
--case_download=false
```


Wait for the cpd-cli to return the following message before proceeding to the next step:

[SUCCESS] ... The apply-cluster-components command ran successfully

Confirm that the Certificate manager pods in the ${PROJECT_CERT_MANAGER} project are Running or Completed:

```
oc get pods --namespace=${PROJECT_CERT_MANAGER}
```

Confirm that the License Service pods are Running or Completed

```
oc get pods --namespace=${PROJECT_LICENSE_SERVICE}
```


## 6. Prepare to upgrade an instance of IBM Software Hub (est. 3 minutes)

Validate the health of your cluster, nodes, operators, and operands before proceeding with the upgrade:

```
cpd-cli health operators \
--control_plane_ns=${PROJECT_CPD_INST_OPERANDS} \
--operator_ns=${PROJECT_CPD_INST_OPERATORS}
```

Results should read "SUCCESS..."

```
cpd-cli health operands \
--control_plane_ns=${PROJECT_CPD_INST_OPERANDS} \
--include_ns=${PROJECT_CPD_INST_OPERATORS}
```

Results should read "SUCCESS..."

```
cpd-cli health cluster
```

Results should read "SUCCESS..."

```
cpd-cli health nodes
```

Results should read "SUCCESS..."

[Next, apply entitlements](https://www.ibm.com/docs/en/software-hub/5.2.x?topic=puish-applying-your-entitlements-3) (est. 1-2 minutes):

Run the apply-entitlement command for each solution that is installed or that you plan to install in this instance

Apply entitlements for CP4D:

```
cpd-cli manage apply-entitlement \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--entitlement=cpd-enterprise
```

Apply entitlements for DataStage:

```
cpd-cli manage apply-entitlement \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--entitlement=datastage
```

Apply entitlements for IKC/WKC:

```
cpd-cli manage apply-entitlement \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--entitlement=ikc-standard
```


## 7. Upgrade an instance of IBM Software Hub

Upgrade the required operators and custom resources for the instance:

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:

```
${CPDM_OC_LOGIN}
```

Review the license terms for the software that you plan to install:

```
cpd-cli manage get-license \
--release=${VERSION} \
--license-type=EE
```

Upgrade the required operators and custom resources for the instance (est. 60 minutes):

```
cpd-cli manage setup-instance \
--license_acceptance=true \
--release=${VERSION} \
--cpd_operator_ns=${PROJECT_CPD_INST_OPERATORS} \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--run_storage_tests=false \
--case_download=false
```

You can optionally run this command with the run_storage_tests flag set to 'true':

```
--run_storage_tests=true
```

Monitor the upgrade progress of the custom resources with the following commands:

```
oc get ZenService lite-cr -n cpd-instance -o yaml
```

```
oc get Ibmcpd ibmcpd-cr -n cpd-instance -o yaml
```

Upgrade the operators in the operators project (est. 32 minutes):

```
cpd-cli manage apply-olm \
--release=${VERSION} \
--cpd_operator_ns=${PROJECT_CPD_INST_OPERATORS} \
--upgrade=true \
--case_download=false
```

Wait for the cpd-cli to return the following message before proceeding to the next step:

[SUCCESS]... The apply-olm command ran successfully

Confirm that the operator pods are Running or Completed:

```
oc get pods --namespace=${PROJECT_CPD_INST_OPERATORS}
```


## 8. Upgrade CPD services (datastage_ent_plus,ws_pipelines,wkc,mantaflow)

### Upgrade WKC custom resource (est. 92 minutes):

Before proceeding with the IKC upgrade steps, confirm that the post-upgrade steps have been completed from the previous 5.2.0 upgrade - [Post-upgrade setup for IBM Knowledge Catalog](https://www.ibm.com/docs/en/software-hub/5.2.x?topic=upgrading-post-upgrade-setup-knowledge-catalog)

After you successfully upgraded IBM Knowledge Catalog from Version 4.8, 5.0, or 5.1 to Version 5.2, you must migrate all IBM Knowledge Catalog data from the previously used Db2 and CouchDB databases to the EDB Native PostgreSQL database that is used in Version 5.2.

After all data that was stored in Db2 is successfully migrated to EDB Postgres, remove all Db2U-related resources from IBM Knowledge Catalog

Verify that the data was migrated without any data loss by checking the migration summary

Get the data-migration pod names:

```
oc get pod -n ${PROJECT_CPD_INST_OPERANDS} | grep data-migration
```

Access the log for a specific pod by using this command. Replace <migration-pod-name> with the name of the pod that was used for migration:

```
oc logs <migration-pod-name>
```

The pod logs contain a migration summary that shows the row counts for each table in Db2 and EDB Postgres, and the total number of rows in either database, similar to these examples:

```
{"message": "33/34 BG.WORKFLOW_ASSET (BGDB): 2,156 rows created in postgres (2,156 rows in DB2)", "timestamp": "2025-07-30 12:16:13,227", "levelname": "INFO"}
{"message": "34/34 BG.SCHEMAVERSION (BGDB): 1 rows created in postgres (1 rows in DB2)", "timestamp": "2025-07-30 12:16:13,227", "levelname": "INFO"}
{"message": "Total number of rows in DB2: 16,931", "timestamp": "2025-07-30 12:16:13,228", "levelname": "INFO"}
{"message": "Total number of rows in Postgres: 16,931", "timestamp": "2025-07-30 12:16:13,228", "levelname": "INFO"}
```

To identify missing data, compare the row counts for each table in Db2 and EDB Postgres

Next, delete the IBM Knowledge Catalog Db2U cluster:

```
oc delete db2ucluster db2oltp-wkc -n ${PROJECT_CPD_INST_OPERANDS}
```

Verify that the Db2U StatefulSet and pod were deleted

To check the StatefulSet:

```
oc get sts -n ${PROJECT_CPD_INST_OPERANDS} | grep db2u
```

To check the pod:

```
oc get pods -n ${PROJECT_CPD_INST_OPERANDS} | grep db2u
```

Next, delete the IBM Knowledge Catalog Db2U PVCs:

```
oc delete pvc c-db2oltp-wkc-meta data-c-db2oltp-wkc-db2u-0 wkc-db2u-backups -n ${PROJECT_CPD_INST_OPERANDS}
```

Delete the IBM Knowledge Catalog Db2U init job:

```
oc delete job wkc-db2u-init -n ${PROJECT_CPD_INST_OPERANDS}
```

Once the Db2U resources have been cleaned up, proceed with the following steps to check that Postgres cluster replicas are synchronized

***Potential Issue Prior to WKC/CCS upgrade - PostgreSQL cluster replicas get out of sync*** - [PostgreSQL cluster replicas get out of sync](https://www.ibm.com/docs/en/cloud-paks/cp-data/5.0.x?topic=platform-postgresql-cluster-replicas-get-out-sync)

This issue can manifest in several ways:
- You see a mismatch in the data between the replicas
- The database cluster is stuck in the Waiting for the instances to become active state

To check the status of the database cluster, run:

```
oc get clusters.postgresql.k8s.enterprisedb.io -n ${PROJECT_CPD_INST_OPERANDS}
```

To confirm that the replicas are out of sync search the PostgreSQL operator logs for the phrase WAL:

```
oc logs -n ${PROJECT_CPD_INST_OPERATORS} \
-l app.kubernetes.io/name=cloud-native-postgresql \
| grep "WAL"
```

Look for messages with the following phrases:
- missing WAL files
- tried restoring WALs

Search the PostgreSQL pod logs for the phrase Failed to execute pg_rewind:

```
oc logs -n ${PROJECT_CPD_INST_OPERANDS} \
-l app.kubernetes.io/name=cloud-native-postgresql \
| grep '"level":"error"' \
| grep 'Failed to execute pg_rewind'
```

Set the POSTGRES_CLUSTER environment variable to the name of the PostgreSQL cluster where you want to check the replica status:

```
export POSTGRES_CLUSTER=<cluster-name>
```

Run the following command to see a list of any PostgreSQL cluster in the project:

```
oc get clusters.postgresql.k8s.enterprisedb.io \
-n ${PROJECT_CPD_INST_OPERANDS}
```

Set the PRIMARY_POD, PRIMARY_POD_LSN, REPLICA_POD, and REPLICA_POD_LSN environment variables:

***Environments with one replica pod***

Set the PRIMARY_POD and REPLICA_POD environment variables:

```
export PRIMARY_POD=$(oc get pod -n ${PROJECT_CPD_INST_OPERANDS} -l k8s.enterprisedb.io/cluster=${POSTGRES_CLUSTER},role=primary -o jsonpath="{.items[0].metadata.name}")
```

```
export REPLICA_POD=$(oc get pod -n ${PROJECT_CPD_INST_OPERANDS} -l k8s.enterprisedb.io/cluster=${POSTGRES_CLUSTER},role=replica -o name)
```

Set the PRIMARY_POD_LSN and REPLICA_POD_LSN environment variables:

```
export PRIMARY_POD_LSN=$(oc exec ${PRIMARY_POD} -n ${PROJECT_CPD_INST_OPERANDS} -c postgres -- psql -U postgres -c "SELECT pg_current_wal_lsn();" | sed -n '3p' | sed -e 's/^[ \t]*//' -e 's/[ \t]*$//')
```

```
export REPLICA_POD_LSN=$(oc -n ${PROJECT_CPD_INST_OPERANDS} exec -t ${REPLICA_POD} $ -c postgres -- psql -U postgres -c "SELECT pg_last_wal_replay_lsn();" | sed -n '3p' | sed -e 's/^[ \t]*//' -e 's/[ \t]*$//')
```

***Environments with multiple replica pods***

Set the PRIMARY_POD environment variable:

```
export PRIMARY_POD=$(oc get pod -n ${PROJECT_CPD_INST_OPERANDS} -l k8s.enterprisedb.io/cluster=${POSTGRES_CLUSTER},role=primary -o jsonpath="{.items[0].metadata.name}")
```

Get the name of the replica pods:

```
oc get pod -n ${PROJECT_CPD_INST_OPERANDS} -l k8s.enterprisedb.io/cluster=${POSTGRES_CLUSTER},role=replica -o name
```

The command returns the list of pods that are associated with the specified PostgreSQL cluster (${POSTGRES_CLUSTER}).

Set the REPLICA_POD environment variable to one of the replica pods:

```
export REPLICA_POD=<pod-name>
```

Set the PRIMARY_POD_LSN and REPLICA_POD_LSN environment variables:

```
export PRIMARY_POD_LSN=$(oc exec ${PRIMARY_POD} -n ${PROJECT_CPD_INST_OPERANDS} -c postgres -- psql -U postgres -c "SELECT pg_current_wal_lsn();" | sed -n '3p' | sed -e 's/^[ \t]*//' -e 's/[ \t]*$//')
```

```
export REPLICA_POD_LSN=$(oc -n ${PROJECT_CPD_INST_OPERANDS} exec -t ${REPLICA_POD} $ -c postgres -- psql -U postgres -c "SELECT pg_last_wal_replay_lsn();" | sed -n '3p' | sed -e 's/^[ \t]*//' -e 's/[ \t]*$//')
```

For each replica, compare the LSN for the primary node and the replica pod:

```
echo $PRIMARY_POD_LSN, $REPLICA_POD_LSN
```

***Solving the Problem***

Check whether the EDB Postgres cluster is fenced:

```
oc get clusters.postgresql.k8s.enterprisedb.io ${POSTGRES_CLUSTER} -o yaml | grep fencedInstances
```

If the EDB Postgres cluster is fenced, lift the fencing:

```
oc annotate clusters.postgresql.k8s.enterprisedb.io ${POSTGRES_CLUSTER} k8s.enterprisedb.io/fencedInstances-
```

Wait several minutes, then check the status of the EDB Postgres cluster:

```
oc get clusters.postgresql.k8s.enterprisedb.io ${POSTGRES_CLUSTER} \
-n ${PROJECT_CPD_INST_OPERANDS}
```

If the status is Cluster in healthy state, no additional resolution is required.

If the status is not Cluster in healthy state, continue to the next step.

Get the name of the persistent volume claim associated with the replica pod:

```
oc describe pod ${REPLICA_POD} -n ${PROJECT_CPD_INST_OPERANDS}
```

In the Volumes section, get the ClaimName. The name of the persistent volume claim is usually the same as the name of the replica pod

Delete the persistent volume claim where the claim name is the same as the replica pod name:

```
oc delete pvc ${REPLICA_POD} -n ${PROJECT_CPD_INST_OPERANDS}
```

Delete the persistent volume claim where the claim name is different from the replica pod name:

```
oc delete pvc <pvc-name> -n ${PROJECT_CPD_INST_OPERANDS}
```

Delete the replica pod:

```
oc delete pod ${REPLICA_POD} -n ${PROJECT_CPD_INST_OPERANDS}
```

After fixing any Postgres cluster replica sync issues you are now ready to upgrade WKC

Set the IKC_TYPE environment variable:

```
export IKC_TYPE=wkc
```

When you upgrade IBM Knowledge Catalog, the options that you specified when you installed IBM Knowledge Catalog are used

Ensure you have specified the correct installation options in the install-options.yml in the cpd-cli work directory (For example: cpd-cli-workspace/olm-utils-workspace/work)

IBM Knowledge Catalog - The following values were extracted from the latest revision of HUK internal runbook:

```
################################################################################
# IBM Knowledge Catalog parameters
################################################################################
custom_spec:
  wkc:
#    enableDataQuality: True
#    enableSemanticAutomation: False
#    enableKnowledgeGraph: True
#    useFDB: True
```

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:

```
${CPDM_OC_LOGIN}
```

Update the custom resource for IBM Knowledge Catalog:

```
cpd-cli manage apply-cr \
--components=${IKC_TYPE} \
--release=${VERSION} \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--param-file=/tmp/work/install-options.yml \
--license_acceptance=true \
--upgrade=true \
--case_download=false
```

***Potential Issue After Initiating WKC Upgrade and CCS Upgrade - jobs-api pod***

1. Get the superuser URI to the jobs-postgres cluster:

```
oc get secret ccs-jobs-postgres-superuser -o jsonpath="{.data.uri}" | base64 -d
```

The output should look like following:

```
postgresql://postgres:{password}@ccs-jobs-postgres-rw.{namespace}:5432/*
```

Replace the /* at the end with /jobsapi, for example:

```
postgresql://postgres:{password}@ccs-jobs-postgres-rw.{namespace}:5432/jobsapi
```

2. Exec into one of the running jobs-api pods and initiate the node interactive shell with the following command:

```
oc get po | grep jobs-api
```

```
oc rsh {jobs-api-pod-name} bash -c "cd /home/node/jobs-api; node"
```

3. Replace the URI value in the below text, copy and paste them into the interactive prompt and press enter:

```
const { Pool } = require("pg");
```

```
const pool = new Pool({ connectionString: "{URI from Step 1}" });
```

```
(await pool.query(`ALTER TYPE state OWNER TO jobsapi;`));
```


4. Press CTRL + D to exit the node interactive shell and type “exit” to exit the jobs-api environment

You may need to recycle the jobs-api pod:

```
oc delete po {jobs-api-pod-name} -n $PROJECT_CPD_INST_OPERANDS
```

The 5.2.1 jobs-api pods should now initialize properly and the IKC upgrade will continue

IBM Knowledge Catalog is upgraded when the apply-cr command returns:

[SUCCESS]... The apply-cr command ran successfully

If you want to confirm that the custom resource status is Completed, you can run the cpd-cli manage get-cr-status command:

```
cpd-cli manage get-cr-status \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--components=${IKC_TYPE} \
--case_download=false
```

Monitor the status of the WKC custom resource:

```
oc get WKC wkc-cr -o yaml
```

When the WKC custom resource upgrade progress reaches 50%, you may observe the following messages:

```
message:
      One or more items failed
      Failed to deploy WKC
      Failed at task: Wait until the service is ready - Item: wdp-policy-service
      The error was: One or more items failed
	  
"Failed to deploy Policy service\nFailed at task:
    Wait until the service is ready - Item: wdp-policy-service\nThe error was: One
    or more items failed"'
```




### Upgrade MANTA Automated Data Lineage custom resource (est. 6 minutes):

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:

```
${CPDM_OC_LOGIN}
```

Run the following commands to remove migration from the MANTA Automated Data Lineage custom resource:

```
oc edit mantaflow mantaflow-wkc
```

Remove the migration section from the cr:

```
  migrations:
    h2-format-3: true
```

Then delete the following deployments from Zen namespace:

```
oc delete deploy manta-admin-gui manta-configuration-service manta-dataflow -n zen
```

Update the custom resource for MANTA Automated Data Lineage:

```
cpd-cli manage apply-cr \
--components=mantaflow \
--release=${VERSION} \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--license_acceptance=true \
--upgrade=true \
--case_download=false
```

Monitor the status of the MANTA custom resource:

```
oc get MantaFlow mantaflow-wkc -o yaml
```

MANTA Automated Data Lineage is upgraded when the apply-cr command returns:

[SUCCESS]... The apply-cr command ran successfully

If you want to confirm that the custom resource status is Completed, you can run the cpd-cli manage get-cr-status command:

```
cpd-cli manage get-cr-status \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--components=mantaflow \
--case_download=false
```


### Upgrade IBM Orchestration Pipelines custom resource (est. 14 minutes):

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:

```
${CPDM_OC_LOGIN}
```

Update the custom resource for IBM Orchestration Pipelines:

```
cpd-cli manage apply-cr \
--components=ws_pipelines \
--release=${VERSION} \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--license_acceptance=true \
--upgrade=true
```

Monitor the status of the Pipelines custom resource:

```
oc get WSPipelines wspipelines-cr -o yaml
```

IBM Orchestration Pipelines is upgraded when the apply-cr command returns:

[SUCCESS]... The apply-cr command ran successfully

If you want to confirm that the custom resource status is Completed, you can run the cpd-cli manage get-cr-status command:

```
cpd-cli manage get-cr-status \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--components=ws_pipelines
```

After Orchestration Pipelines upgrade is complete, apply the Orchestration Pipelines 5.2.1 HotFix

Create a backup of the current catalog image:

```
export PROJECT_CPD_OPERATORS=cpd-operators
```

```
oc -n $PROJECT_CPD_OPERATORS get CatalogSources ibm-cpd-wspipelines-operator-catalog -o=jsonpath="{.spec.image}"
```

Export the following environment variables:

```
export PROJECT_CPD_INSTANCE=cpd-instance
```

```
export PROJECT_CPD_OPERATORS=cpd-operators
```

Patch Orchestration Pipelines catalog source:

```
oc patch catsrc -n ${PROJECT_CPD_OPERATORS} ibm-cpd-wspipelines-operator-catalog --type='json' -p='[{"op": "replace", "path": "/spec/image", "value":"icr.io/cpopen/ibm-cpd-wspipelines-operator-catalog@sha256:295eeca47512295a840bc174b7448ebcc5d2e79dee87805478217d12e3a99f94"}]'
```

Verify that the catalog pod is up and running:

```
oc get pods -n ${PROJECT_CPD_OPERATORS} | grep ibm-cpd-wspipelines-operator-catalog
```

Delete subscription and CSV:

```
oc delete sub -n ${PROJECT_CPD_OPERATORS} ibm-cpd-wspipelines-operator-catalog-subscription
```

```
oc delete csv -n ${PROJECT_CPD_OPERATORS} ibm-cpd-wspipelines.v11.1.0
```

Recreate subscription to initiate installation, but make sure to replace <PROJECT_CPD_OPERATORS> with the cpd operators project:

```
cat << EOF | oc apply -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: ibm-cpd-wspipelines-operator-catalog-subscription
  namespace: <PROJECT_CPD_OPERATORS>
spec:
  channel: v11.1
  installPlanApproval: Automatic
  name: ibm-cpd-wspipelines
  source: ibm-cpd-wspipelines-operator-catalog
  sourceNamespace: <PROJECT_CPD_OPERATORS>
EOF
```

Wait a few minutes for CSV to be ready:

```
oc -n ${PROJECT_CPD_OPERATORS} get csv ibm-cpd-wspipelines.v11.1.0 -o jsonpath="{.status.phase}"
```

Output should be:

```
Succeeded
```

Add the hotfix label to the CSV:

```
oc label csv ibm-cpd-wspipelines.v11.1.0 -n $PROJECT_CPD_OPERATORS support.operator.ibm.com/hotfix=true
```

To verify the newly-added label, please check:

```
oc get csv ibm-cpd-wspipelines.v11.1.0 -n $PROJECT_CPD_OPERATORS -o jsonpath='{.metadata.labels}'
```

Restart wsp-ts pods using command:

```
oc -n ${PROJECT_CPD_INSTANCE} rollout restart deploy wsp-ts
```

Wait 15 to 20 minutes for reconciliation process to finish:

```
oc -n ${PROJECT_CPD_INSTANCE} get wspipelines
```

Output should be:

```
NAME             VERSION   RECONCILED   STATUS       PERCENT   AGE
wspipelines-cr   5.2.1     5.2.1        Completed    100%      35m
```

### Upgrade DataStage custom resource (est. 11 minutes):

***Potential Issue During DataStage Custom Resource Upgrade***

During the previous upgrade we encountered an issue in which the DataStage operator pod was crashing, for example:

```
oc get po -n $PROJECT_CPD_INST_OPERATORS | grep datastage-operator
```

```
ibm-cpd-datastage-operator-665cd9c4d8-9mxw9                       0/1     CrashLoopBackOff     8          77h
```

Upon closer inspection of the datastage operator pod, we discovered the pod was crashing due to an OOMKilled error:

```
oc describe pod ibm-cpd-datastage-operator-665cd9c4d8-9mxw9 -n $PROJECT_CPD_INST_OPERANDS
```

Under the status -> conditions section, look for any errors, such as OOMKilled

In order to address this, increase the datastage operator pod memory by updating the memory requests/limits in the CSV:

```
oc get csv -A | grep datastage
```

```
oc edit csv ibm-cpd-datastage-operator.v8.0.0
```

Increase the memory limit from 1Gi to 2Gi:

```
resources:
  limits:
	cpu: 500m
	memory: 2Gi

```

Wait a few minutes for the changes to be applied before proceeding with the upgrade

Set the ${DATASTAGE_TYPE} variable to the edition of DataStage that you want to install:

```
export DATASTAGE_TYPE=datastage_ent_plus
```

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:

```
${CPDM_OC_LOGIN}
```

Update the custom resource for DataStage:

```
cpd-cli manage apply-cr \
--components=${DATASTAGE_TYPE} \
--release=${VERSION} \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--license_acceptance=true \
--upgrade=true \
--case_download=false
```


Monitor the status of the DataStage custom resource:

```
oc get DataStage datastage -o yaml
```

DataStage is upgraded when the apply-cr command returns:

[SUCCESS]... The apply-cr command ran successfully

Note: The service instances are automatically upgraded when you upgrade DataStage

If you want to confirm that the custom resource status is Completed, you can run the cpd-cli manage get-cr-status command:

```
cpd-cli manage get-cr-status \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--components=datastage_ent_plus
```

After DataStage upgrade is complete, apply DataStage 5.2.1 Patch 2

Run the following command for the datastage instance to apply the patch:

```
oc patch datastage datastage -n "${PROJECT_CPD_INST_OPERANDS}" --type merge -p '{"spec": {"image_digests": {"canvas": "sha256:02c83d398472796cda042ebd11058f32c4677263fd1150b165673c23ec74de3c","caslite": "sha256:ad2be55d3704bacc04c4c2bda1ebe3e39d1f53dd1351612c25802051346a9e3a","flows": "sha256:a4c5cd3316d6f7d3ce348801ae4d9bcaa8934c1993a601f14a2baa5e02cf1ea7","nginx": "sha256:fc655812543610c6d1ae76554e8b9927aff51110566d9d3189bafdb96ebfb30e","migration": "sha256:ccce0b203e73f8f1c5b79f9cd20e6b27098c75e4f64f4719defa4893f010fa82","assets": "sha256:ee14e70938e957ececcf0bc96a61f987502f7db9f378be87bef91ef5e9ca5d55","ruleset": "sha256:3ca33516b56a8d4a8e0fa3a8acf37f0c4531060504b42b7c4f8159b00c17612c","runtime": "sha256:9cfa40397bfb255efa3a5e5b3bfbc51ce918a27e7818c7817fe096709849b4d2","dataservice": "sha256:6134f3c8c0cc654e4c8109d73c49818bd085184c7558c2429dee241e29b64ee4","metrics": "sha256:5e61fe899c4e6f4ff3d16d60312a6e91147d5e5ba3df9a589b5692fe81549c69"}}}'
```

Wait for the DataStage operator reconciliation to complete

Run the following command to monitor the reconciliation status:

```
oc get datastage datastage -n ${PROJECT_CPD_INST_OPERANDS}
```

After some time, the respective datastage pods in ${PROJECT_CPD_INST_OPERANDS} namespace should be up and running with the updated image

Note that some images will not update based on if the previous patches were applied

Find the pxruntime instance names:

```
oc get pxruntime -n ${PROJECT_CPD_INST_OPERANDS}
```

Run the following command for each instance found in the above command to apply the patch:

```
oc patch pxruntime -n ${PROJECT_CPD_INST_OPERANDS} [instance-name] --type merge -p '{"spec":{"image_digests":{"pxcompute":"sha256:ee7c858ccca5877cb55e1b0f004e014b92713c669a2608780d64e25704eb8ba2","pxruntime":"sha256:c34704017a6af51cf5601f1cba5cf579602bf3bdcb3423fdd16bb1899069b6e3"}}}'
```

Wait for the DataStage operator reconciliation to complete. Run the following command to monitor the reconciliation status:

```
oc get pxruntime [instance-name] -n ${PROJECT_CPD_INST_OPERANDS}
```

After some time, the datastage px-runtime and px-compute pods in ${PROJECT_CPD_INST_OPERANDS} should be up and running with the updated image

If you are upgrading from pre-5.2.1 to 5.2.1 release, perform these additional steps to regenerate configmaps after the upgrade is completed

Delete the existing configmaps:

```
oc delete cm datastage-maint-aux-ckpt-cm -n ${PROJECT_CPD_INST_OPERANDS}
```

```
oc delete cm datastage-maint-aux-br-cm -n ${PROJECT_CPD_INST_OPERANDS}
```

Wait a few minutes for the configmaps to be regenerated and then run the following command to monitor the newly generated configmaps:

```
oc get cm -n cpd-instance | egrep "datastage-maint-aux-ckpt|datastage-maint-aux-br"
```

This marks the end of the datastage upgrade procedure


### Post-upgrade of all custom resources (est. <1 minute):

After all custom resources have been updated, enable the zen-rsi-evictor-cron-job CronJob:

```
oc patch CronJob zen-rsi-evictor-cron-job \
--namespace=${PROJECT_CPD_INST_OPERANDS} \
--type=merge \
--patch='{"spec":{"suspend": false}}'
```


## 9. Upgrade the cpdbr service:

Upgrade the cpdbr-tenant component for the instance

The command that you run depends on the type of storage that you use, where the cluster pulls images from, and whether the scheduling service is installed

For IBM Fusion environments without the scheduling service, run the following command:

```
cpd-cli oadp install \
--component=cpdbr-tenant \
--cpdbr-hooks-image-prefix=${PRIVATE_REGISTRY_LOCATION} \
--tenant-operator-namespace=${PROJECT_CPD_INST_OPERATORS} \
--upgrade=true \
--log-level=debug \
--verbose \
--case_download=false
```


## 10. Upgrade privileged monitors (est. 2 minutes)

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:

```
${CPDM_OC_LOGIN}
```

Upgrade the privileged monitors

The command that you run depends on whether you want the Operator namespace status check to include information about the scheduling service

To include only information about the operators project run the following command:

```
cpd-cli manage apply-privileged-monitoring-service \
--privileged_service_ns=${PROJECT_PRIVILEGED_MONITORING_SERVICE} \
--cpd_operator_ns=${PROJECT_CPD_INST_OPERATORS} \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--case_download=false
```


## 11. Upgrade the configuration admission controller webhook (est. 2 minutes):

Log the cpd-cli in to the Red Hat® OpenShift® Container Platform cluster:

```
${CPDM_OC_LOGIN}
```

Upgrade the configuration admission controller webhook for x86_64 clusters:

```
cpd-cli manage install-cpd-config-ac \
--cpd_instance_ns=${PROJECT_CPD_INST_OPERANDS} \
--cpd_config_ac_image=${PRIVATE_REGISTRY_LOCATION}/cpopen/cpd/zen-rsi-adm-controller:${VERSION}.amd64 \
--case_download=false
```


## 12. Validate CPD upgrade (customer acceptance test):

End of document
